---
sidebar_position: 3
---

# Testing the RAG

In order to validate that the **Retrieval-Augmented Generation (RAG)** system is providing accurate and relevant responses from your sources, you can test the chat in our **Testing Widget**. This helps ensure your developers receive the correct information when interacting with the AI-powered assistants deployed through **Layer**.

## How to Test Chat Responses

1. **Access the Testing Chat Widget**: Currently, you can test the chat responses by going to the **Web Widget** page under the **Deployments** section in the Management Dashboard. This chat widget will simulate real developer interactions and show how the system responds using the configured sources.

2. **Evaluate Responses**: Ask sample questions relevant to your documentation and see how the system responds. Ensure the answers are accurate and based on the sources you've provided.

## Addressing Response Issues

If the responses are not as expected, you can improve them by either:

- **Adding or Adjusting Sources**: If the chat assistant is missing information, you may need to add new sources or adjust existing ones. Check the **Managing Sources** page for more details on how to manage and configure your sources.
- **Adjusting the RAG Configuration**: If the responses are still not accurate, you may need to tweak your **RAG configuration**. Visit the **RAG Configuration** page for more options on refining how your system generates responses.

In a future release, **Layer** will make the testing chat widget more accessible throughout the entire Management Dashboard, enabling you to test responses more easily from any page.

## Next Steps

- **Manage Sources**: If testing reveals gaps or inaccuracies in your responses, visit the [Managing Sources](#) page to add or adjust your documentation sources.
- **Adjust RAG Configuration**: For more advanced response tuning, explore the [RAG Configuration](#) page to fine-tune your setup.
