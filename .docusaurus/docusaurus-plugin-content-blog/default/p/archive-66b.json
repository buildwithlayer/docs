{"archive":{"blogPosts":[{"id":"llm_extensibility","metadata":{"permalink":"/llm_extensibility","source":"@site/blog/llm_extensibility.mdx","title":"LLM Extensibility: Unifying Brands, Developers, and AI Surfaces","description":"Layer AI Blog | What Is LLM Extensibility?","date":"2025-01-28T00:00:00.000Z","tags":[{"inline":true,"label":"LLM Extensibility","permalink":"/tags/llm-extensibility"}],"readingTime":6.065,"hasTruncateMarker":false,"authors":[{"name":"Jonah Katz","title":"Co-Founder and CEO @ Layer","url":"https://www.linkedin.com/in/jonahkatz1/","imageURL":"https://media.licdn.com/dms/image/v2/D5603AQGcdwug9LunVA/profile-displayphoto-shrink_400_400/profile-displayphoto-shrink_400_400/0/1727827703156?e=1743638400&v=beta&t=S4adBtUI5ELMYeuV-xjKvHl5lB8k6IaQ6T6gXhVvuuk","key":"jkatz","page":null},{"name":"Andrew Hamilton","title":"Co-Founder and CTO @ Layer","url":"https://github.com/andrewlayer","imageURL":"https://avatars.githubusercontent.com/u/135887157?v=4","key":"aham","page":null}],"frontMatter":{"slug":"llm_extensibility","title":"LLM Extensibility: Unifying Brands, Developers, and AI Surfaces","authors":["jkatz","aham"],"date":"2025-01-28T00:00:00.000Z","descripion":"What is LLM Extensibility? Learn how devs, brands, and AI surfaces unify their workflows for real-time, reliable AI—without fragmented code or docs.","tags":["LLM Extensibility"],"keywords":["LLM Extensibility"]},"unlisted":false,"nextItem":{"title":"Keep Your Data DRY with APIFlask","permalink":"/dry_with_api_flask"}},"content":"<head>\n  <title>Layer AI Blog | What Is LLM Extensibility?</title>\n</head>\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n## The Trust Gap in AI-Assisted Coding\n\nAsk any developer who’s turned to an AI chatbot for coding help, and you’ll likely hear the same question, reverberating at various volumes in their heads: *“Can I really trust this snippet?”*\n\nYou never know if the chatbot is scraping outdated docs, merging conflicting tutorials, or pulling random code from unmaintained repos. Worse, you’re forced to juggle multiple platforms—IDE, browser, chat window—just to test and integrate those suggestions.\n\nNobody wants to waste hours verifying code that was supposed to save them time. That’s where LLM Extensibility steps in, bringing curated, brand-approved-and-controlled data that stays in sync across every AI environment you rely on. The result? A coding assistant you can actually trust, with fewer late-night debugging sessions and far less copy-paste chaos.\n\n<Tabs>\n  <TabItem value=\"standard\" label=\"Standard LLM Coding Assistance\" default>\n   <div style={{\"text-align\": \"center\"}}>\n        ![Standard LLM Coding Assistance](./assets/llm_extensibility/standard_llm_coding_assistance.png)\n    </div>\n  </TabItem>\n  <TabItem value=\"extensibility\" label=\"LLM Extension Coding Assistance\" >\n    <div style={{\"text-align\": \"center\"}}>\n        ![LLM Extension Coding Assistance](./assets/llm_extensibility/llm_extension_coding_assistance.png)\n    </div>\n  </TabItem>\n</Tabs>\n\n## What Is LLM Extensibility—and Why Does It Matter?\n\n**LLM Extensibility** is the process of embedding brand-approved, continuously updated knowledge and functionality directly into AI environments—so that developers, businesses, and AI platforms can collaborate more effectively. Instead of letting tools like ChatGPT or GitHub Copilot scrape the open web for partial information, LLM Extensibility ensures they always draw from the most reliable, real-time sources.\n\nThis means a single, consistent pipeline: when a company publishes official documentation, code samples, or best practices into an “LLM extension,” developers see the correct info wherever they use AI—whether that’s in VS Code, Copilot, ChatGPT, Anthropic, or any other AI surface. No guesswork, no stale references, and a drastically simpler way to trust the AI’s output.\n\n![What is LLM Extensibility?](./assets/llm_extensibility/what_is_llm_extensibility.png)\n\n## Everybody Wins: Unifying Brands, Developers, and AI Surfaces\n\nWhen your API is constantly evolving (think Stripe, Plaid, Neon, or any fast-moving platform), it’s no small feat to keep docs consistent across various AI tools. Developers often piece together partially-hallucinated code snippets with trial and error, leading to bugs, confusion, and extra support overhead. LLM Extensibility solves that by offering a single source of truth for docs, code samples, and best practices—across every AI environment that matters. \n1. **Brands** gain direct control over how popular AI tools assist developers with their APIs and SDKs. By building extensions for ecosystems like ChatGPT, Claude, and GitHub Copilot, they embed up-to-date, brand-approved content and functionality right where developers live—ensuring consistency, reliability, and a smoother path to integration.\n2. **Developers** can trust that what they’re pulling is always current and aligned with official best practices. Plus, the AI can do more than advise: it can open pull requests, run tests, or spin up resources in the developer’s environment, guided by reliable content.\n3. **AI Surfaces** (VS Code, ChatGPT, GitHub Copilot, Anthropic, etc.) deliver a richer, more consistent user experience. Instead of scraping partial data and generating hallucinated code, they plug into a universal source of truth that updates in real time and brings powerful agentic functionality to their users.\n\nOf course, not every AI platform has embraced extensibility yet. Some, like Cursor, remain closed off—for now. But as the [ecosystem around LLM extensibility grows](https://publish.obsidian.md/andrew-vault/Layer/Blogs/LLM+Extensibility-+Race+for+an+Ecosystem), user expectations will evolve, and every surface will likely open up to brand-specific extensions to stay competitive.\n\n## Why Not Just “Ask ChatGPT”?\n\nIf ChatGPT (or another super powerful LLM) can browse the web, why bother with a brand-managed extension? The short answer is **reliability and control.**\n\nOpen-web crawling might yield half-correct references or piecemeal code that no longer matches a brand’s latest version. An official extension, on the other hand, feeds the AI precisely what the brand wants it to see—nothing more, nothing less.\n\nFor developers, that means fewer misfires. Instead of sifting through questionable snippets, they can rely on curated, brand-sanctioned responses. Couple that with the AI’s ability to manipulate files in your IDE or open pull requests in GitHub, and you’ve got an active collaborator rather than a passive advisor. You still oversee or approve changes, but the manual copy-paste grind disappears.\n\n## One Extension, Many Surfaces—Build Once, Publish Everywhere\n\nYes, a company could build separate plugins for each AI environment—[VS Code extension](https://code.visualstudio.com/api/extension-guides/chat), [Github Copilot extension](https://github.com/marketplace?type=apps&copilot_app=true), [OpenAI GPT](https://chatgpt.com/gpts), [Anthropic MCP Server](https://github.com/modelcontextprotocol/servers)—and manually update each one whenever their docs or OpenAPI spec changes. But that approach is time-consuming and prone to version drift.\n\nA **single publishing model** makes far more sense: create one extension (containing your official docs, code samples, or agentic actions), then **deploy** it to whichever AI surfaces you want to support. Whenever you update your docs or add new features, every environment reflects the change at once. It’s akin to how React Native lets you build an app once and distribute it to multiple platforms (iOS, Android, MacOS, Windows, and Web); here, you’re uploading and maintaining **brand** content and logic on one platform, and distributing to multiple AI tools with no code required.\n\nThis is the vision behind **platforms like Layer**, which aim to unify LLM Extensibility. Rather than building piecemeal integrations for each environment, you create a single extension—your “source of truth”—and publish it across supported surfaces from one central dashboard. Brands update content once, developers find consistent, up-to-date docs and agentic tools, and AI surfaces reap the benefits of official, high-quality knowledge.\n\n<Tabs>\n  <TabItem value=\"fragmented\" label=\"Fragmented Distribution Model\" default>\n   <div style={{\"text-align\": \"center\"}}>\n        ![Fragmented Distribution Model](./assets/llm_extensibility/fragmented_distribution_model.png)\n    </div>\n  </TabItem>\n  <TabItem value=\"unified\" label=\"Unified Distribution Model\" >\n    <div style={{\"text-align\": \"center\"}}>\n        ![Unified Distribution Model](./assets/llm_extensibility/unified_distribution_model.png)\n    </div>\n  </TabItem>\n</Tabs>\n\n## Insights and Efficiency for Everyone\n\nOne huge benefit of LLM Extensibility is the **insight** it provides. Brands that control their extension can track real-world usage: which queries come up most often, which features confuse developers, and where the biggest gaps lie. That feedback loop shapes future documentation tweaks and even core product decisions.\n\nDevelopers, meanwhile, can streamline their workflows. They no longer hunt down the right doc version or wonder if a snippet is still valid; the AI always references the latest info. And AI surfaces gain a reputation for **trustworthy** guidance, pulling from an official source rather than stitching together random web scraps.\n\n## Where AI Goes from Here\n\nWe’re already seeing signs that AI can do more than suggest code—it can act. Opening pull requests, provisioning services, or orchestrating CI/CD pipelines are all becoming part of an LLM’s repertoire. **LLM Extensibility** paves the way for that evolution by grounding these actions in brand-approved data and logic. And as more AI surfaces become extensible, the line between “AI advice” and “AI-driven automation” continues to blur.\n\nThat’s good news for everyone in this conversation: brands, developers, and AI platform providers. With a unified extensibility model, changes happen once, code is consistently accurate, and developers can do more with less friction. Instead of scraping questionable snippets or juggling plugin updates, the future looks a lot more **connected**—and a lot more **trustworthy.**\n\nThat’s the essence of **LLM Extensibility:** a blueprint for AI that respects brand control, fosters developer confidence, and unlocks richer, continuously updated capabilities across all the surfaces where work actually happens. If you’re ready to leave behind scattered docs and fragmented plugin strategies, this could be the next big step toward a smarter, more seamless AI pipeline."},{"id":"dry_with_api_flask","metadata":{"permalink":"/dry_with_api_flask","source":"@site/blog/dry_with_api_flask.mdx","title":"Keep Your Data DRY with APIFlask","description":"Lead Author: Gavyn","date":"2024-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"DRY","permalink":"/tags/dry"},{"inline":true,"label":"APIFlask","permalink":"/tags/api-flask"},{"inline":true,"label":"SQLAlchemy","permalink":"/tags/sql-alchemy"},{"inline":true,"label":"Marshmallow","permalink":"/tags/marshmallow"},{"inline":true,"label":"OneOfSchema","permalink":"/tags/one-of-schema"}],"readingTime":12.515,"hasTruncateMarker":false,"authors":[{"name":"Gavyn Partlow","title":"Software Engineer @ Layer","url":"https://github.com/GavynWithLayer","imageURL":"https://avatars.githubusercontent.com/u/151399782?v=4","key":"gpart","page":null},{"name":"Lucas Gismondi","title":"Software Engineer @ Layer","url":"https://github.com/lucasgismondi-gg","imageURL":"https://avatars.githubusercontent.com/u/121511329?v=4","key":"lgis","page":null},{"name":"Andrew Hamilton","title":"Co-Founder and CTO @ Layer","url":"https://github.com/andrewlayer","imageURL":"https://avatars.githubusercontent.com/u/135887157?v=4","key":"aham","page":null}],"frontMatter":{"slug":"dry_with_api_flask","title":"Keep Your Data DRY with APIFlask","authors":["gpart","lgis","aham"],"date":"2024-12-20T00:00:00.000Z","descripion":"Lead by Gavyn, Co-authored by Lucas and Andrew","tags":["DRY","APIFlask","SQLAlchemy","Marshmallow","OneOfSchema"]},"unlisted":false,"prevItem":{"title":"LLM Extensibility: Unifying Brands, Developers, and AI Surfaces","permalink":"/llm_extensibility"}},"content":"<small>\n**Lead Author:** Gavyn<br/>\n**Co-Authors:** Lucas, Andrew<br/>\n\nAssociated Repository: [blog-dry_api_flask_demo](https://github.com/buildwithlayer/blog-dry_api_flask_demo)<br/>\n</small>\n\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n## ☔️ When it starts to rain \nWhen working with a traditional [Model/View/Controller](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller) approach, \nit is easy to fall suspect to code duplication. I've\nseen it with coworkers, friends, and even family. No one is safe from code duplication. However, there are some tips and\ntricks you can use with Flask to help protect yourself and your loved ones.\n\n## Data Sources\nFirst, let's talk about where data comes from and how it can trick us into making the same models multiple times.\n\n### Database + SQLAlchemy\n\nThe main source of data in most backends is a database (it's in the name). If you've been around the block and done more\nthan a few python APIs, you're probably familiar with tools like [Flask](https://flask.palletsprojects.com/en/) and [SQLAlchemy](https://www.sqlalchemy.org/). SQLAlchemy is great to help you\nmodel and manage data in your database without ever writing a line of SQL, and that's every developer's dream.\n\nWhen working with Flask and SQLAlchemy, you'll often see ORM models like this:\n\n\n```python\nclass Farmer(db.Model):\n    id: Mapped[int] = mapped_column(\n        Integer, primary_key=True,\n    )\n    created_at: Mapped[DateTime] = mapped_column(\n        DateTime, nullable=False, server_default=func.now(),\n    )\n    updated_at: Mapped[DateTime] = mapped_column(\n        DateTime, nullable=False, server_default=func.now(), onupdate=func.now(),\n    )\n    name: Mapped[str] = mapped_column(\n        String, nullable=False,\n    )\n```\n\nAnd this is great! You've got an abstraction of the columns in your `farmer` table. Not only can you read, create,\nupdate, and delete your farmers from your database with ease, but you can also make changes to the table itself, and\nSQLAlchemy will help you migrate your data. Very developer friendly and very useful!\n\n### APIs + Marshmallow\n\nThe next source of data in any API backend is the APIs themselves! You've got two categories of data: requests and\nresponses. In many cases, developers follow a model/view/controller pattern, and the GET routes are returning something\nnearly identical to the ORM model.\n\nLet's extend our example:\n\n```python\nfarmers_bp = APIBlueprint(\n    \"farmers\", __name__, enable_openapi=True\n)\n\n# Marshmallow Schema\nclass FarmerOut(Schema):\n    id = fields.Integer(required=True)\n    created_at = fields.DateTime(required=True)\n    updated_at = fields.DateTime(required=True)\n    name = fields.String(required=True)\n\n# Flask Route\n@farmers_bp.get(\"/<int:farmer_id>\")\n@farmers_bp.output(FarmerOut)\ndef get_farmer_by_id(farmer_id: int):\n    farmer = Farmer.query.where(Farmer.id == farmer_id).first()\n    if farmer is None:\n        raise HTTPError(404, message=\"Farmer not found\")\n    return farmer\n```\n\nNow if there exists a record in our database, we can ping `farmers/1` and get the following response:\n\n```js\n{\n  \"created_at\": \"2023-12-12T15:51:00\",\n  \"id\": 1,\n  \"name\": \"Old MacDonald\",\n  \"updated_at\": \"2023-12-12T15:51:00\"\n}\n```\n\n### 🌊 Monsoon Season \n\nThe well-seasoned developer might dust off their salt and pepper and say, \"Wait! I've seen those same fields before!\"\nAnd they'd be right! Looking at the `Farmer` class and the `FarmerOut` class, the fields are nearly identical.\n\n\n<Tabs>\n  <TabItem value=\"orange\" label=\"SQLAlchemy\" default>\n    \n    ```python\n    # SQLAlchemy Schema\n    class Farmer(db.Model):\n        id: Mapped[int] = mapped_column(Integer, primary_key=True)\n        created_at: Mapped[DateTime] = mapped_column(DateTime, nullable=False, server_default=func.now())\n        updated_at: Mapped[DateTime] = mapped_column(DateTime, nullable=False, server_default=func.now(), onupdate=func.now())\n        name: Mapped[str] = mapped_column(String, nullable=False)\n    ```\n  </TabItem>\n  <TabItem value=\"apple\" label=\"Marshmallow\" >\n    ```python\n    # Marshmallow Schema\n    class FarmerOut(Schema):\n        id = fields.Integer(required=True)\n        created_at = fields.DateTime(required=True)\n        updated_at = fields.DateTime(required=True)\n        name = fields.String(required=True)\n    ```\n  </TabItem>\n</Tabs>\n\nThis is definitely a bad look. Imagine if we were to add a new field to the `Farmer` class? Or even more sneaky, change\nthe type of one of the fields? We'd then have to update `FarmerOut` and any other schemas we may have in the future that\ninclude `Farmer` to match. This is a burden on developers, but it also is a chance for subtle bugs to creep in.\n\n## Buy 1, Get 1 Free!\n\nThankfully, we have some tools at our disposal to help avoid this kind of disaster. Enter [`SQLAlchemyAutoSchema`](https://marshmallow-sqlalchemy.readthedocs.io/en/latest/api_reference.html), stage\nleft. Let's look at how we can use [`flask-marshmallow`](https://flask-marshmallow.readthedocs.io/en/latest/) and `SQLAlchemyAutoSchema` to help avoid all this duplication.\n\n### Simple Example\n\nBelow our `Farmer` definition, we can add a new class for the `FarmerSchema` as follows:\n\n```python\nclass FarmerSchema(marsh.SQLAlchemyAutoSchema):\n    class Meta:\n        model = Farmer\n```\n\nThen, we just update our route to use this new schema:\n\n```python\n@farmers_bp.get(\"/<int:farmer_id>\")\n@farmers_bp.output(FarmerSchema) # <-- Updated\ndef get_farmer_by_id(farmer_id: int):\n    farmer = Farmer.query.where(Farmer.id == farmer_id).first()\n    if farmer is None:\n        raise HTTPError(404, message=\"Farmer not found\")\n    return farmer\n```\n\nAnd now, if we were to ping the same request as before, we get the same response! This is thanks to the\n`SQLAlchemyAutoSchema` automatically parsing all the properties of the associated `model` (passed in its `Meta` class).\nThis means any new fields added to our ORM model will be automatically added to our schema!\n\n### Relationships\n\nLet's add a new ORM model that has a many-to-one relationship with the `Farmer`, such as chickens.\n\n<Tabs>\n  <TabItem value=\"image\" label=\"Image\" default>\n    <div style={{\"text-align\": \"center\"}}>\n        ![farmer-chicken-schema](./assets/dry_with_api_flask/farmer_chicken.png)\n    </div>\n  </TabItem>\n  <TabItem value=\"code\" label=\"Code\">\n  \n    ```python\n    class Sex(enum.Enum):\n        MALE = \"male\"\n        FEMALE = \"female\"\n\n\n    class Chicken(db.Model):\n        id: Mapped[int] = mapped_column(\n            Integer, primary_key=True,\n        )\n        created_at: Mapped[DateTime] = mapped_column(\n            DateTime, nullable=False, server_default=func.now(),\n        )\n        updated_at: Mapped[DateTime] = mapped_column(\n            DateTime, nullable=False, server_default=func.now(), onupdate=func.now(),\n        )\n        farmer_id: Mapped[int] = mapped_column(\n            Integer, ForeignKey(\"farmer.id\", ondelete=\"CASCADE\"),\n        )\n        age: Mapped[int] = mapped_column(\n            Integer, nullable=False,\n        )\n        sex: Mapped[Sex] = mapped_column(\n            Enum(Sex), nullable=False,\n        )\n\n\n    class ChickenSchema(marsh.SQLAlchemyAutoSchema):\n        class Meta:\n            model = Chicken\n    ```\n    \n  </TabItem>\n</Tabs>\n\nOh no, it's starting to rain. We have duplication on some of our fields in the model (`id`, `created_at`, `updated_at`),\nbut we are seasoned developers, and we know we can just abstract that out to a `BaseModel` of sorts. No biggie!\n\n<Tabs>\n    <TabItem value=\"image\" label=\"Image\" default>\n        <div style={{\"text-align\": \"center\"}}>\n            ![farmer-chicken-schema](./assets/dry_with_api_flask/farmer_chicken_base.png)\n        </div>\n    </TabItem>\n    <TabItem value=\"code\" label=\"Code\">\n\n    ```python\n    class BaseModel(db.Model):\n        id: Mapped[int] = mapped_column(\n            Integer, primary_key=True,\n        )\n        created_at: Mapped[DateTime] = mapped_column(\n            DateTime, nullable=False, server_default=func.now(),\n        )\n        updated_at: Mapped[DateTime] = mapped_column(\n            DateTime, nullable=False, server_default=func.now(), onupdate=func.now(),\n        )\n\n        # --- METADATA ---\n        __abstract__ = True\n    ```\n\n    </TabItem>\n</Tabs>\nAnd then we just inherit from the `BaseModel` for both `Farmer` and `Chicken`. Easy! The `Farmer` class is looking very\nsimple now, which is good.\n\n```python\nclass Farmer(BaseModel):\n    name: Mapped[str] = mapped_column(\n        String, nullable=False,\n    )\n\n    # --- RELATIONSHIPS ---\n    chickens: Mapped[List[Chicken]] = relationship(\n        \"Chicken\", cascade=\"all, delete\",\n    )\n```\n\nBut what about the duplication of the `Schema` classes we are making? They are the same each time, except the\n`Meta.model` points to whichever model the schema belongs to. How could we extract this out to reduce duplication? Well,\nknow that we have a `BaseModel`, let's just give it a `classmethod` that generates our `Schema` class for us!\n\n```python\nclass BaseMeta(object):\n    include_relationships = True\n\n\nclass BaseModel(db.Model):\n    ...\n    __schema__ = None\n    \n    @classmethod\n    def make_schema(cls) -> type(SQLAlchemyAutoSchema):\n        if cls.__schema__ is not None:\n            return cls.__schema__\n        \n        meta_kwargs = {\n            \"model\": cls,\n        }\n        meta_class = type(\"Meta\", (BaseMeta,), meta_kwargs)\n        \n        schema_kwargs = {\n            \"Meta\": meta_class\n        }\n        schema_name = f\"{cls.__name__}Schema\"\n        \n        cls.__schema__ = type(schema_name, (SQLAlchemyAutoSchema,), schema_kwargs)\n        return cls.__schema__\n```\n\nThis is a pretty crafty method that creates a customer `Meta` class for the given `cls`, and then uses that in a custom\n`SQLAlchemyAutoSchema` class, which is then returned. We can now set the `FarmerSchema` and `ChickenSchema` as follows:\n\n```python\nFarmerSchema = Farmer.make_schema()\nChickenSchema = Chicken.make_schema()\n```\n\nNow, let's add a couple of chickens for the farmer in our database, and test out the same endpoint. Here is the\nresponse:\n\n```js\n{\n  \"chickens\": [\n    1,\n    2\n  ],\n  \"created_at\": \"2023-12-12T15:51:00\",\n  \"id\": 1,\n  \"name\": \"Old MacDonald\",\n  \"updated_at\": \"2023-12-12T15:51:00\"\n}\n```\n\nWhat's going on here? We have the `include_relationships` property in `FarmerSchema.Meta`, so why are we only getting\nthe `id` of each `Chicken`? Unfortunately, the way to get composition relationships in `marshmallow.Schema` is through\n`Nested` fields. There is no auto translation of `SQLAlchemy.relationship()` to `marshmallow.fields.Nested`, but we are\nclever developers, right? We can figure something out.\n\n```python\nclass BaseModel(db.Model):\n    ...\n    @classmethod\n    def get_relationship(cls, attr_name: str) -> Optional[Relationship]:\n        attr = getattr(cls, attr_name)\n        prop = getattr(attr, \"property\", None)\n        if prop is None or not isinstance(prop, Relationship):\n            return None\n        return prop\n    \n    @classmethod\n    def nest_attribute(cls, attr_name: str, prop: Relationship, schema_kwargs: dict):\n        many = getattr(prop, \"collection_class\", None) is not None\n        entity = getattr(prop, \"entity\", None)\n        nested_class = getattr(entity, \"class_\", None)\n        if not hasattr(nested_class, \"make_schema\"):\n            raise TypeError(f\"Unexpected nested type [{type(nested_class).__name__}]\")\n\n        schema_kwargs[attr_name] = fields.Nested(\n            nested_class.make_schema()(many=many)\n        )\n    \n    @classmethod\n    def make_schema(cls) -> type(SQLAlchemyAutoSchema):        \n        ... # same as before\n\n        # Add relationships to the schema\n        for attr_name in cls.__dict__:\n            if (prop := cls.get_relationship(attr_name)) is not None:\n                cls.nest_attribute(attr_name, prop, schema_kwargs)\n\n        cls.__schema__ = type(schema_name, (SQLAlchemyAutoSchema,), schema_kwargs)\n        return cls.__schema__\n```\n\nThis new `make_schema()` method will automatically detect any fields that are `SQLAlchemy.Relationships`, and convert\nthem to the appropriate `marshmallow.fields.Nested()` as long as the class inherits from `BaseModel`. Pretty nifty!\n\nNow, if we make the same request as before, let's see what we get:\n\n> TypeError: Object of type Sex is not JSON serializable\n\nNot the first time I've heard that. Let's see what we can do to fix this. The issue is very similar to the relationship\nvs. nested problem we saw before. `SQLAlchemy` has one notion of an `Enum`, while `marshmallow` has another. We can do a\nsimilar conversion within our `make_schema` function as follows:\n\n```python\nclass BaseModel(db.Model):\n    ... # same as before\n    @classmethod\n    def get_enum(cls, attr_name: str) -> Optional[Type[Enum]]:\n        attr = getattr(cls, attr_name)\n        attr_type = getattr(attr, \"type\", None)\n        if attr_type is None:\n            return None\n\n        return getattr(attr_type, \"enum_class\", None)\n\n    @classmethod\n    def enum_attribute(cls, attr_name: str, enum_class: Type[Enum], schema_kwargs: dict):\n        schema_kwargs[attr_name] = fields.Enum(enum_class)\n\n    @classmethod\n    def make_schema(cls) -> type(SQLAlchemyAutoSchema):\n        ... # same as before\n\n        for attr_name in cls.__dict__:\n            if (prop := cls.get_relationship(attr_name)) is not None:\n                cls.nest_attribute(attr_name, prop, schema_kwargs)\n            elif (enum_class := cls.get_enum(attr_name)) is not None:\n                cls.enum_attribute(attr_name, enum_class, schema_kwargs)\n\n        cls.__schema__ = type(schema_name, (SQLAlchemyAutoSchema,), schema_kwargs)\n        return cls.__schema__\n```\n\nNow, when we make the same request, we get:\n\n```js\n{\n  \"chickens\": [\n    {\n      \"age\": 3,\n      \"created_at\": \"2023-12-12T18:17:53\",\n      \"id\": 1,\n      \"sex\": \"MALE\",\n      \"updated_at\": \"2023-12-12T18:17:53\"\n    },\n    {\n      \"age\": 2,\n      \"created_at\": \"2023-12-12T18:46:30\",\n      \"id\": 2,\n      \"sex\": \"FEMALE\",\n      \"updated_at\": \"2023-12-12T18:46:30\"\n    }\n  ],\n  \"created_at\": \"2023-12-12T15:51:00\",\n  \"id\": 1,\n  \"name\": \"Old MacDonald\",\n  \"updated_at\": \"2023-12-12T15:51:00\"\n}\n```\n\n### Polymorphism\n\nNow that our relationships are healthy, we can move to the next step: polymorphism! Let's say we don't want to just keep\ntrack of farmers and their livestock, but also their crops! Well, `SQLAlchemy` has us covered with its `__mapper_args__`\nmetadata and the `polymorphic` fields of that object!\n\nFor our purposes, we want one generic `Crop` model that keeps track of the type of crop, the maturity time, and how many\nacres a farmer has of that crop.\n<Tabs>\n  <TabItem value=\"image\" label=\"Image\" default>\n    ![Crop Image](./assets/dry_with_api_flask/farmer_chick_crops.png)\n  </TabItem>\n  <TabItem value=\"code\" label=\"Code\">\n    ```python\n    class Crop(BaseModel):\n        farmer_id: Mapped[int] = mapped_column(\n            Integer, ForeignKey(\"farmer.id\", ondelete=\"CASCADE\"), nullable=False,\n        )\n        type: Mapped[str] = mapped_column(\n            String, nullable=False,\n        )\n        days_to_mature: Mapped[int] = mapped_column(\n            Integer, nullable=False,\n        )\n        acres: Mapped[float] = mapped_column(\n            Float, nullable=False,\n        )\n\n        # --- METADATA ---\n        __mapper_args__ = {\n            \"polymorphic_identity\": \"crop\",\n            \"polymorphic_on\": \"type\",\n        }\n\n\n    class Cucumber(Crop):\n        id: Mapped[int] = mapped_column(\n            Integer, ForeignKey(\"crop.id\", ondelete=\"CASCADE\"), primary_key=True,\n        )\n        for_pickling: Mapped[bool] = mapped_column(\n            Boolean, default=False, nullable=False,\n        )\n\n        # --- METADATA ---\n        __mapper_args__ = {\"polymorphic_identity\": \"cucumber\"}\n\n\n    class Tomato(Crop):\n        id: Mapped[int] = mapped_column(\n            Integer, ForeignKey(\"crop.id\", ondelete=\"CASCADE\"), primary_key=True,\n        )\n        diameter: Mapped[float] = mapped_column(\n            Float, nullable=False,\n        )\n\n        # --- METADATA ---\n        __mapper_args__ = {\"polymorphic_identity\": \"tomato\"}\n    ```\n  </TabItem>\n\n\n</Tabs>\nNow, we also want to move all of our schema declarations into their own `schemas` module. After doing that, we create\nthe `CucumberSchema` and `TomatoSchema` as normal:\n\n```python\nCucumberSchema = Cucumber.make_schema()\nTomatoSchema = Tomato.make_schema()\n```\n\nEverything is looking good, but there is trouble on the horizon. If we look at the generated schema for the `Farmer`,\nsomething is off. The `crops` field says it is a list of `CropSchemas`, but this is only partially true. Ideally, the\n`crops` field should be a list of either `TomatoSchemas` or `CucumberSchemas`.\n\n## The Magic of OneOfSchema\n\nThankfully, there is already an extension to help us solve this problem; itroducing [`marshmallow_oneofschema`](https://github.com/marshmallow-code/marshmallow-oneofschema)!\n\n### Polymorphism II: Even DRYer\n\nTo use the `OneOfSchema` class for our `CropSchema`, we just have to do the following:\n\n```python\nclass CropSchema(OneOfSchema):\n    type_schemas: Dict[str, str] = {\n        \"cucumber\": CucumberSchema,\n        \"tomato\": TomatoSchema,\n    }\n\n    type_field_remove = False\n\n    def get_obj_type(self, obj: Crop):\n        return obj.type\n```\n\nThe `type_schemas` property is a mapping of the `type` field of a given `Crop` to which schema it should use when\nserializing or deserializing. It's that simple! Unfortunately, this has one drawback when implementing into our given\nstack: `make_schema()` does not know of `CropSchema's` existence. When creating the `FarmerSchema`, it will deduce the\nclass of the `crops` field, which is `Crop`, and then it will call `Crop.make_schema()` to get the nested schema.\n\nThis is no good! What can we do to fix this? Overrides.\n\n```python\nclass BaseModel(db.Model):\n    ... # same as before\n    @classmethod\n    def make_schema(cls, overrides: Optional[Dict[str, fields.Field]] = None) -> type(SQLAlchemyAutoSchema):\n        ... # same as before\n\n        for attr_name in cls.__dict__:\n            if attr_name in overrides:\n                schema_kwargs[attr_name] = overrides[attr_name]\n            elif (prop := cls.get_relationship(attr_name)) is not None:\n                cls.nest_attribute(attr_name, prop, schema_kwargs)\n            elif (enum_class := cls.get_enum(attr_name)) is not None:\n                cls.enum_attribute(attr_name, enum_class, schema_kwargs)\n\n        cls.__schema__ = type(schema_name, (SQLAlchemyAutoSchema,), schema_kwargs)\n        return cls.__schema__\n```\n\nThis way, when we create the `FarmerSchema`, we can tell it specifically to use the polymorphic `CropSchema` for the\n`crops` field.\n\n```python\nFarmerSchema = Farmer.make_schema(\n    overrides={\"crops\": fields.Nested(CropSchema(), many=True)}\n)\n```\n\nNow, when we call our endpoint, we get:\n\n```js\n{\n  \"chickens\": [\n    {\n      \"age\": 3,\n      \"created_at\": \"2023-12-12T18:17:53\",\n      \"id\": 1,\n      \"sex\": \"MALE\",\n      \"updated_at\": \"2023-12-12T18:17:53\"\n    },\n    {\n      \"age\": 2,\n      \"created_at\": \"2023-12-12T18:46:30\",\n      \"id\": 2,\n      \"sex\": \"FEMALE\",\n      \"updated_at\": \"2023-12-12T18:46:30\"\n    }\n  ],\n  \"created_at\": \"2023-12-12T15:51:00\",\n  \"crops\": [\n    {\n      \"acres\": 1,\n      \"created_at\": \"2023-12-12T20:21:32\",\n      \"days_to_mature\": 60,\n      \"for_pickling\": true,\n      \"id\": 1,\n      \"type\": \"cucumber\",\n      \"updated_at\": \"2023-12-12T20:21:32\"\n    },\n    {\n      \"acres\": 0.5,\n      \"created_at\": \"2023-12-12T20:22:07\",\n      \"days_to_mature\": 80,\n      \"diameter\": 3,\n      \"id\": 2,\n      \"type\": \"tomato\",\n      \"updated_at\": \"2023-12-12T20:22:07\"\n    }\n  ],\n  \"id\": 1,\n  \"name\": \"Old MacDonald\",\n  \"updated_at\": \"2023-12-12T15:51:00\"\n}\n```\n\nBeautiful and dry! Like a sunny day! ☀️\n\n## Mechanics (AKA Auto-Docs)\n\nA fantastic feature of APIFlask is that it conforms to the OpenAPI spec with its routes and schemas. This means we've\nactually been documenting our APIs the whole time as we write them! Here are the docs:\n\n<iframe\n        id=\"apiflask-docs\"\n        title=\"APIFLask Docs\"\n        src=\"https://dry-apiflask-demo-r5uz5svela-uc.a.run.app/docs#/\"\n        width=\"100%\"\n        height=\"500px\"\n>\n</iframe>\n\n\n### The First 90%\n\nIf you look around the auto generated docs, you'll see the routes that we made, as well as the schemas that are in use.\nOne quick change I'd suggest is to try out all the different UIs available for the docs site. You can update this by\nsetting the `docs_ui` key-word argument in the `APIFlask` constructor like so:\n\n```python\nAPIFlask(__name__, title=\"DRY API\", version=\"1.0\", docs_ui=\"elements\")\n```\n\nDevelopers with sharp eyes may notice that the `Crop` schema doesn't have any information populated in our docs! This is\na problem.\n\n### The Last 10%\n\nThe final savior: [`apispec_oneofschema`](https://github.com/timakro/apispec-oneofschema), a companion to `marshmallow_oneofschema`. This plugin allows us to generate\ndocumentation for our `OneOfSchema` schemas. Let's set it up now!\n\nIt's as simple as changing this:\n\n```python\napp = APIFlask(__name__, title=\"DRY API\", version=\"1.0\", docs_ui=\"elements\")\n```\n\nTo this:\n\n```python\napp = APIFlask(__name__, title=\"DRY API\", version=\"1.0\", docs_ui=\"elements\", spec_plugins=[MarshmallowPlugin()])\n```\n\n### The last 1%\n\nLastly, the `oneOf` dropdown for most of the UIs just says `object` for each option, which isn't great. From what I can\ntell, most of the UIs use the `title` field of a schema to populate the name, so we can create our own plugin to add\nthat field for each of our schemas:\n\n```python\nfrom apispec.ext import marshmallow\n\n\nclass OpenAPITitleAppender(marshmallow.OpenAPIConverter):\n    def schema2jsonschema(self, schema):\n        json_schema = super(OpenAPITitleAppender, self).schema2jsonschema(schema)\n        schema_name = schema.__class__.__name__\n        if schema_name.endswith('Schema'):\n            schema_name = schema_name[:-len('Schema')]\n        json_schema[\"title\"] = schema_name\n        return json_schema\n\n\nclass TitlesPlugin(marshmallow.MarshmallowPlugin):\n    Converter = OpenAPITitleAppender\n```\n\nAnd then we just have to add it to our `APIFlask` app!\n\n```python\napp = APIFlask(\n    __name__,\n    title=\"DRY API\",\n    version=\"1.0\",\n    docs_ui=\"elements\",\n    spec_plugins=[MarshmallowPlugin(), TitlesPlugin()]\n)\n```"}]}}