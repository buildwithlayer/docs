{"archive":{"blogPosts":[{"id":"vscode_grubhub_extension","metadata":{"permalink":"/vscode_grubhub_extension","source":"@site/blog/vscode_grubhub_extension.mdx","title":"Building a VSCode Chat Extension to Order Me Cheeseburgers","description":"I built a VS Code chat extension that orders food from Grubhub using AI-driven API calls. Follow along as I reverse-engineer Grubhub‚Äôs API, integrate it with VS Code, and accidentally order myself a cheeseburger in the process.","date":"2025-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"LLM Extensibility","permalink":"/tags/llm-extensibility"},{"inline":true,"label":"API","permalink":"/tags/api"},{"inline":true,"label":"Developer Experience (DX)","permalink":"/tags/developer-experience-dx"},{"inline":true,"label":"Agent Experience (AX)","permalink":"/tags/agent-experience-ax"},{"inline":true,"label":"AI Agents","permalink":"/tags/ai-agents"},{"inline":true,"label":"VS Code","permalink":"/tags/vs-code"},{"inline":true,"label":"Grubhub","permalink":"/tags/grubhub"}],"readingTime":8.595,"hasTruncateMarker":false,"authors":[{"name":"Andrew Hamilton","title":"Co-Founder and CTO @ Layer","url":"https://github.com/andrewlayer","imageURL":"https://avatars.githubusercontent.com/u/135887157?v=4","key":"aham","page":null}],"frontMatter":{"id":"vscode_grubhub_extension","slug":"vscode_grubhub_extension","title":"Building a VSCode Chat Extension to Order Me Cheeseburgers","authors":["aham"],"date":"2025-02-27T00:00:00.000Z","description":"I built a VS Code chat extension that orders food from Grubhub using AI-driven API calls. Follow along as I reverse-engineer Grubhub‚Äôs API, integrate it with VS Code, and accidentally order myself a cheeseburger in the process.","tags":["LLM Extensibility","API","Developer Experience (DX)","Agent Experience (AX)","AI Agents","VS Code","Grubhub"],"keywords":["LLM Extensibility","API","Developer Experience (DX)","Agent Experience (AX)","AI Agents","VS Code","Grubhub"]},"unlisted":false,"nextItem":{"title":"AI Go-To-Market: Why Agents Are Your Newest Path to Adoption","permalink":"/ai_gtm"}},"content":"<head>\n  <title>Layer AI Blog | Building a VSCode Chat Extension to Order Me Cheeseburgers</title>\n  <meta property=\"og:title\" content=\"Layer AI Blog | Building a VSCode Chat Extension to Order Me Cheeseburgers\" />\n</head>\n\nAre you ever in the sigma developer grindset so hard that you forget to eat? Me neither. But like most VC backed companies, I will attempt to solve a problem that does not exist! In the process I hope you learn how to waste time as well as me while still getting paid.\n\n![Burger](./assets/vscode_grubhub_extension/burger.webp)\n\n## Where do we start? \n\nLet's look at the sophisticated technical architecture I will be using to accomplish this feat of engineering.\n\nHere is the flow:\n1. **VSCode Chat API:** Developer asks copilot to \"Order lunch\"\n2. **LLM Determines tool Calls:** GET_LUNCH_OPTIONS\n3. **Copilot Responds:** Copilot will list options from the restaurant for what they can order\n4. **Developer responds:** \"Cheeseburger\"\n5. **LLM Determines tool Calls:** ORDER_LUNCH_ITEM\n6. **Copilot Responds:** \"Your cheeseburger has been ordered sir\"\n\n## Reverse Engineering Grubhub API like a Sigma Developer\n\nAt first I wanted to use Doordash, but they use server-side rendering to display their menus which would make our jobs real hard.  So I settled on using Grubhub instead (and may I just say, this was the correct choice).  Now Grubhub doesn't have a public API for ordering food (they do have this [API](https://developer.grubhub.com/), but this is for merchants of which I am not), so we need to reverse engineer the API.  To do this I used Chrome Dev Tools & [Postman Interceptor](https://chromewebstore.google.com/detail/postman-interceptor/aicmkgpgakddgnaphhhpliifpcfhicfo?hl=en).  \n\n### My first \"accidental\" cheeseburger\n\nIn order to intercept all the requests, I needed to reverse engineer the API: I had to place an order.  So with my postman interceptor listening and the company card details ready, I walked through the checkout process and clicked \"Submit\".  Suddenly hundreds of requests poured out of my computer. I then rapidly tried to cancel the order, but it was too late. The food arrived at my house 30 minutes later. Here it is in its full glory:\n\n```\n                            |\\ /| /|_/|\n                          |\\||-|\\||-/|/|\n                           \\\\|\\|//||///\n          _..----.._       |\\/\\||//||||\n        .'     o    '.     |||\\\\|/\\\\ ||\n       /   o       o  \\    | './\\_/.' |\n      |o        o     o|   |          |\n      /'-.._o     __.-'\\   |          |\n      \\      `````     /   |          |\n      |``--........--'`|    '.______.'\n       \\              /\n\t    `'----------'`\n\nI forgot to take a picture, enjoy this ascii art\n```\n\n\n\nThe burger was as good as I had imagined it would be, especially since it was on the company dime. But more importantly, I had all the request information I needed to start the reverse engineering.  All in all it took me about 8 hours to distill only the necesary requests for placing and order. I found it only takes 4 POST and 1 PUT request on Grubhub to make an order.  To save you all the time, here they are:\n\n1. **POST `/carts`:** This route creates a new cart on the user's account\n2. **POST `/carts/{cart_id}/lines`:** This allows us to add an item to the cart we just created\n3. **PUT `/carts/{cart_id}/delivery_info`:** This updates the delivery address for the cart\n4. **POST `/carts/{cart_id}/payments`:** This attaches a payment method to the cart\n5. **POST `/carts/{cart_id}/checkout`:** This places the order\n\nHonestly, looking at it now, I am a bit disappointed that this took me 8 hours to figure out.  Now there are a few more routes we are going to add to make the VSCode checkout experience smoother, but these 5 routes are all you need to place an order using the Grubhub API.  You can find them in this postman collection if you are interested (nerd).\n\n## VSCode Extension\n\nSo what next?  Well this header says \"VSCode Extension\", so I guess we can talk about that.  VSCode extensions are a bunch of TypeScript accessing a bunch of [APIs](https://code.visualstudio.com/api/get-started/your-first-extension).  You can actually start one with a single command here:\n\n```\nnpx --package yo --package generator-code -- yo code`\n```\n\nNow you could start from scratch with the command above, but I suggest you just clone my [Grubhub project](https://github.com/andrewlayer/grubhub) and remove what you don't want. \n\nLet us step back for a moment and take a look at the full project structure:\n\n![VS Code Grubhub Extension Project Structure](./assets/vscode_grubhub_extension/vscode_extension_diagram.png)\n\nIf you take a close look at the diagram, you can see there are two parts to our extension.  The stuff that VSCode requires for us to render a participant. And the stuff required to call the tools / use the LLM.  For the former, I will refer you to these [docs](https://code.visualstudio.com/api/extension-guides/chat-tutorial) as they are pretty good.  The latter will be what I focus this blogpost on.\n\n### How do we call an API with an LLM?\n\nSo function calling basically works like this:\n\n<div style={{display: \"flex\", flexDirection: \"column\", gap: \"16px\"}}>\n    <div style={{padding: \"20px\", backgroundColor: \"#333\", borderRadius: \"10px\"}}>\n        <p style={{margin: \"0px\", fontWeight: \"bold\", color: \"#58A6FF\"}}>User:</p>\n        <p style={{margin: \"0px\"}}>Hey LLM I have this function called add that takes parameters `{num1: int, num2: int}` only respond with JSON so I can parse it from the response. Please add 5 and 9</p>\n    </div>\n    <div style={{padding: \"20px\", backgroundColor: \"#333\", borderRadius: \"10px\"}}>\n        <p style={{margin: \"0px\", fontWeight: \"bold\", color: \"#12B981\"}}>Assistant:</p>\n        <code>{`{num1: 5, num2: 9}`}</code>\n    </div>\n</div>\n<br/>\n\nWhile the LLMs which produce these JSON schemas no longer need to be prompted as such, fundamentally this is how function calling works. Getting LLMs to produce domain specific languages is actually a super interesting concept, but we are trying to get some burgers üçî üçî üçî.\n\n\nHere is an example of one of the tool schemas for `/get_restaurant_items`:\n```json\n\"inputSchema\": {\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"restaurant_id\": {\n\t\t\t\"type\": \"string\",\n\t\t\t\"description\": \"The ID of the restaurant\"\n\t\t}\n\t},\n\t\"required\": [\"restaurant_id\"]\n}\n\n--> Expected response from LLM\n{\n  \"restaurant_id\": \"38427391\"\n}\n```\n\nThis response it easy to parse with `JSON.loads()` and then can be validated with something like [Zod](https://zod.dev) and [Pydantic](https://pydantic.dev) to ensure it is correct.  These tool schemas are declared in the `package.json` file in the extension which you can find [here](https://github.com/andrewlayer/grubhub/blob/main/package.json)!\n\n### Function Calling ‚òéÔ∏è\nSo now that we have our JSON, we need to use it to invoke a function. In the case of calling an API endpoint, that means we need to take our parameters and shove them into javascript `fetch`.  Here is how we got that done for `/get_restaurant_items`:\n\n```typescript\nexport class GetRestaurantItemsTool implements vscode.LanguageModelTool<GetRestaurantItemsParameters> {\n    async invoke(\n        options: vscode.LanguageModelToolInvocationOptions<GetRestaurantItemsParameters>,\n        _token: vscode.CancellationToken\n    ) {\n        try {\n            const res = await grubhubClient.getRestaurantItems(options.input.restaurant_id);\n            \n            const itemsList = response.items.map(item => \n                `- ${item.item_name} (ID: ${item.item_id})\\n\n                  ${item.item_description || 'No description available'}`\n            ).join('\\n\\n');\n\n            return new vscode.LanguageModelToolResult([\n                new vscode.LanguageModelTextPart(\n                    itemsList || 'No items found'\n                )\n            ]);\n        } catch (error) {\n            return new vscode.LanguageModelToolResult([\n                new vscode.LanguageModelTextPart(\n                    `Failed to get restaurant items: ${error instanceof Error ? error.message : 'Unknown error'}`\n                )\n            ]);\n        }\n    }\n}\n```\n\nIn the code above, we implement the `vscode.LanguageModelTool` class which requires the `invoke` function.  This is ultimately what does the \"calling\" of the tool. In this line here:\n\n```ts\nconst res = await grubhubClient.getRestaurantItems(options.input.restaurant_id);\n```\n\nYou can see we get the restaurant ID.  You might be asking, \"but sir üßê how did you parse the JSON?\".  Well, you see, by implementing the language model tool class, this is done automatically for me as long as I provide a JSON schema!\n\n## Workflows (a quick aside)\nNow in order to make any agentic experience nice, you really need workflows.  *Why is this?* Well, let me show you a hypothetical conversation and see if you understand:\n\n<div style={{display: \"flex\", flexDirection: \"column\", gap: \"16px\"}}>\n    <div style={{padding: \"20px\", backgroundColor: \"#333\", borderRadius: \"10px\"}}>\n        <p style={{margin: \"0px\", fontWeight: \"bold\", color: \"#58A6FF\"}}>Hungry Developer:</p>\n        <p style={{margin: \"0px\"}}>Hey can you list my restaurants</p>\n    </div>\n    <div style={{padding: \"20px\", backgroundColor: \"#333\", borderRadius: \"10px\"}}>\n        <p style={{margin: \"0px\", fontWeight: \"bold\", color: \"#12B981\"}}>AI (internally panicking):</p>\n        <p style={{margin: \"0px\"}}>You need to make a session first before I can list your restaurants, let me do that.</p>\n        <p style={{margin: \"0px\", fontStyle: \"italic\", color: \"#777\"}}>(frantically making API calls in the background)</p>\n    </div>\n    <div style={{padding: \"20px\", backgroundColor: \"#333\", borderRadius: \"10px\"}}>\n        <p style={{margin: \"0px\", fontWeight: \"bold\", color: \"#58A6FF\"}}>Still Hungry Developer:</p>\n        <p style={{margin: \"0px\"}}>ok can you do it now please</p>\n    </div>\n    <div style={{padding: \"20px\", backgroundColor: \"#333\", borderRadius: \"10px\"}}>\n        <p style={{margin: \"0px\", fontWeight: \"bold\", color: \"#12B981\"}}>AI (sweating):</p>\n        <p style={{margin: \"0px\"}}>Getting your favorite restaurants here they are:</p>\n        <br/>\n        ```\n        ‚Ä¢ Restaurant 123421\n        ‚Ä¢ Restaurant 60552\n        ‚Ä¢ Restaurant 666\n        ```\n        <p style={{margin: \"0px\", fontStyle: \"italic\", color: \"#777\"}}>(nailed it! ...right?)</p>\n    </div>\n    <div style={{padding: \"20px\", backgroundColor: \"#333\", borderRadius: \"10px\"}}>\n        <p style={{margin: \"0px\", fontWeight: \"bold\", color: \"#58A6FF\"}}>Hangry Developer:</p>\n        <p style={{margin: \"0px\"}}>What?? I want the names of the restaurants, not their IDs üò°</p>\n    </div>\n    <div style={{padding: \"20px\", backgroundColor: \"#333\", borderRadius: \"10px\"}}>\n        <p style={{margin: \"0px\", fontWeight: \"bold\", color: \"#12B981\"}}>AI (having an existential crisis):</p>\n        <p style={{margin: \"0px\"}}>Ahhh I see, I need to get the names using this route for each ID. Here they are:</p>\n        <br/>\n        ```\n        ‚Ä¢ Beighley's Burgers and Bananas\n        ‚Ä¢ Jared's Jive\n        ‚Ä¢ Dave's Delicious Driveway\n        ```\n        <p style={{margin: \"0px\", fontStyle: \"italic\", color: \"#777\"}}>(phew, crisis averted... until the next API call)</p>\n    </div>\n</div>\n<br/>\n\nThe above conversation is the actual flow of API calls required for Copilot to list restaurants for Grubhub (moderately dramatized). This obviously isn't very user friendly. You see, most APIs our of the box are not ready to be used by AI agents because they provide bad UX and require additional information that us as users (and LLMs) don't care about.  Thus we must clean and simplify the API\n\nSo how can we accomplish these workflow. Well in this project, I hardcode them all.  But if you are interested in effortlessly cleaning your API for agents to use effectively... Allow me to introduce you to [Layer.](https://buildwithlayer.com/).\n\n## Gosh are we done yet?\nFor the most part, yes. But don't you want to order some food? \n\n1. **Install the extension** [here](https://marketplace.visualstudio.com/items?itemName=buildwithlayer.grubhub).  This will open a tab in VSCode where you can then actually add the extension. \n\n2. **Get your bearer token & POINT**: Alright so I didn't handle auth well, this took me too long anyways.  you can get your bearer token and POINT by intercepting the `https://api-gtm.grubhub.com/restaurants/availability_summaries` request made as such:<br/>![Grubhub Bearer](./assets/vscode_grubhub_extension/grubhub_bearer.png)\n\n3. Input those values into the VSCode Grubhub extension settings:<br/>![VSCode Settings](./assets/vscode_grubhub_extension/vscode_settings.png)\n\n4. Restart VSCode et voil√† üéâ!\n\nYou can now use the VSCode chat extension!  If for some reason you like my content, feel free to subscribe to my [newsletter](https://docs.google.com/forms/d/e/1FAIpQLSf4WGoin0w1eM5f9cwZ4kyd7ocx9i2uAmMVNPAj80_prSUHDg/viewform?usp=dialog).  I promise to sell your email to the highest bidder! (just kidding, it will stay with me)."},{"id":"ai_gtm","metadata":{"permalink":"/ai_gtm","source":"@site/blog/ai_gtm.mdx","title":"AI Go-To-Market: Why Agents Are Your Newest Path to Adoption","description":"Discover how AI agents are reshaping the go-to-market strategy for APIs. Learn why making your API agent-friendly with AI-powered tools like GitHub Copilot and prompt-to-app builders is crucial for reaching new users and driving adoption.","date":"2025-02-25T00:00:00.000Z","tags":[{"inline":true,"label":"AI Go-to-Market","permalink":"/tags/ai-go-to-market"},{"inline":true,"label":"LLM Extensibility","permalink":"/tags/llm-extensibility"},{"inline":true,"label":"API","permalink":"/tags/api"},{"inline":true,"label":"Developer Experience (DX)","permalink":"/tags/developer-experience-dx"},{"inline":true,"label":"Agent Experience (AX)","permalink":"/tags/agent-experience-ax"},{"inline":true,"label":"AI Agents","permalink":"/tags/ai-agents"}],"readingTime":5.755,"hasTruncateMarker":false,"authors":[{"name":"Jonah Katz","title":"Co-Founder and CEO @ Layer","url":"https://www.linkedin.com/in/jonahkatz1/","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQH2LNM38t56gw/profile-displayphoto-shrink_400_400/B4DZUFQFoiHwAg-/0/1739549859357?e=1745452800&v=beta&t=_d4NnBv-PKurwk7BTGq6qFBoHQhwvZzBS9_zagNjpeE","key":"jkatz","page":null}],"frontMatter":{"id":"ai_gtm","slug":"ai_gtm","title":"AI Go-To-Market: Why Agents Are Your Newest Path to Adoption","authors":["jkatz"],"date":"2025-02-25T00:00:00.000Z","description":"Discover how AI agents are reshaping the go-to-market strategy for APIs. Learn why making your API agent-friendly with AI-powered tools like GitHub Copilot and prompt-to-app builders is crucial for reaching new users and driving adoption.","tags":["AI Go-to-Market","LLM Extensibility","API","Developer Experience (DX)","Agent Experience (AX)","AI Agents"],"keywords":["AI Go-to-Market","LLM Extensibility","API","Developer Experience (DX)","Agent Experience (AX)","AI Agents"]},"unlisted":false,"prevItem":{"title":"Building a VSCode Chat Extension to Order Me Cheeseburgers","permalink":"/vscode_grubhub_extension"},"nextItem":{"title":"How Layer Built Neon‚Äôs Copilot Extension","permalink":"/layer_neon_copilot_extension"}},"content":"<head>\n  <title>Layer AI Blog | AI Go-To-Market: Why Agents Are Your Newest Path to Adoption</title>\n  <meta property=\"og:title\" content=\"Layer AI Blog | AI Go-To-Market: Why Agents Are Your Newest Path to Adoption\" />\n  <meta property=\"og:image\" content=\"https://storage.googleapis.com/generic-assets/blog-static-assets/ai_gtm.png\" />\n</head>\n\nFor years I‚Äôve seen companies zeroing in on **developer experience**‚Äîcrafting better docs, building language-specific SDKs, reducing friction in signups. But now, the biggest shift I‚Äôm seeing isn‚Äôt just about developers; it‚Äôs about AI. If you have an API or dev tool, it‚Äôs no longer humans alone reading your docs or signing up on your dashboard.\n\nIncreasingly, it‚Äôs **AI agents**‚Äîlike GitHub Copilot, Cursor, or more autonomous ‚Äúprompt-to-app‚Äù builders‚Äîspinning up resources and writing code automatically, often from inside IDEs. And if those AI agents don‚Äôt recognize your product or can‚Äôt easily use it, you‚Äôre out of the running before a human even knows you exist.\n\nThis is why I call it **AI Go-To-Market:** The path that used to run through documentation, blog posts, or word-of-mouth now runs straight through AI agents. In some scenarios‚Äîparticularly with ‚Äúprompt-to-app‚Äù builders like Lovable‚Äîagents can even decide which API to integrate, potentially bypassing your product if it‚Äôs not AI-friendly. Meanwhile, other tools (like Copilot or Cursor) handle tasks for developers who never even need to see your UI. So unless you adapt to these agentic workflows, you risk missing out on net-new usage and customers you never knew existed.\n\n![AI Go-to-Market](./assets/ai_gtm/ai_gtm.png)\n\n## A New Persona: The AI Agent\n\nI‚Äôve started thinking of AI agents as a brand-new persona‚Äîakin to a developer, but lacking all the human intuition. Agents rely on structured instructions, frictionless signups, and machine-friendly docs to parse your product. As Resend Founder Zeno Rocha [notes](https://resend.com/blog/agent-experience), agentic AI tools ‚Äúrely on an LLM-readable format, like llms.txt, to operate efficiently.‚Äù So if your platform is confusing or locked behind manual forms, the agent moves on‚Äîoften without you realizing you lost that opportunity.\n\nI‚Äôve seen this play out firsthand. At Layer, we build Copilot extensions (and other AI integrations) for companies that want to leverage AI agents to scale their go-to-market motion. When we worked with Neon, for instance, [we built a GitHub Copilot extension](https://docs.buildwithlayer.com/layer_neon_copilot_extension) so devs could simply type ‚Äú@Neon‚Äù to create or manage their databases‚Äîright in Copilot‚Äôs chat. Many of them never even opened Neon‚Äôs dashboard. **That‚Äôs AI go-to-market in action:** the agentic workflow delivers brand-new usage directly from GitHub Copilot.\n\nThis shift can be surprising‚Äîboth to us and to the companies investing in AI GTM. Netlify, for instance, is [seeing over 1,000 new sites created daily](https://biilmann.blog/articles/introducing-ax/) through their ChatGPT plugin‚Äîshowing how agents bypass many ‚Äútraditional‚Äù sign-up flows. We used to assume human-readable docs were the key primitive for driving API adoption. Now, we see AI agents taking a lead role in ‚Äúusing‚Äù and ‚Äúselling‚Äù products, often bypassing the usual onboarding steps. It‚Äôs a new funnel that can make API onboarding and usage dramatically more frictionless than anything we‚Äôve seen.\n\n## Why ‚ÄúAgentic‚Äù Tools Are Changing GTM\n\nInstead of developers manually exploring your site, an AI agent tries to interpret your endpoints, figure out authentication, and deploy code for the user. Tools like Copilot or Cursor are already speeding up coding for experienced devs, while more autonomous ‚Äúprompt-to-app builders‚Äù like [Lovable](https://lovable.dev) and [Bolt](https://bolt.new) let anyone type in a sentence and watch the AI wire up entire full-stack apps. Some companies quietly gain usage because an AI spontaneously integrated them; others lose out because they never built agent-facing logic in the first place.\n\n:::note[Agent Experience: Early Builders in AX]\n\nA handful of dev-tool companies are already tailoring their platforms for **agentic AI**. [Netlify‚Äôs CEO](https://biilmann.blog/articles/introducing-ax/) introduced the term **Agent Experience (AX)** to describe APIs that can be parsed and acted upon by AI with minimal human oversight. From [Stytch](https://stytch.com/blog/the-age-of-agent-experience/) focusing on secure logins for agents to Neon enabling AI-driven Postgres database tasks, and from [Convex](https://convex.dev/) shipping LLM-friendly docs to [Netlify](https://www.netlify.com/) and [Resend](https://resend.com/blog/agent-experience) rethinking frictionless onboarding‚Äîthese teams are making agent readiness a core product principle. Meanwhile, [Anima](https://www.animaapp.com/) bridges design-to-code workflows so AI can generate front-ends, [Mastra](https://mastra.ai/) offers a TypeScript agent framework for building multi-agent systems, and [Liveblocks](https://liveblocks.io/blog) explores collaborative features that AI tools can tap into.\n\nAll of this is coming together at [agentexperience.ax](https://agentexperience.ax/), a community where tech founders and AI engineers are sharing practical tips, open standards, and real-world examples of ‚ÄúLLM-ready‚Äù integration. Whether it‚Äôs measuring AX success or incorporating open protocols, the collective goal is to help AI agents navigate and utilize platforms more autonomously. It‚Äôs a growing movement that underscores a key insight: if you want your product to appear on an AI agent‚Äôs radar, your API needs to be agent-friendly from the ground up.\n:::\n\n## IDEs vs. Prompt-to-App Builders\n\nI break the ‚Äúagentic AI‚Äù environment into two main categories.\n\nFirst, there are **AI-enhanced IDEs** (e.g., GitHub Copilot, Cursor) that help devs code faster by generating snippets or letting them type ‚Äú@YourProduct‚Äù to do tasks. This might not yield massive net-new signups, but it‚Äôs fantastic for removing friction and improving DX for devs already considering your tool.\n\nSecond, there are **prompt-to-app builders** that can bring real net-new adoption, because they auto-generate entire projects or workflows. If your product is recognized as the best fit, the AI just picks it‚Äîoften without the user manually researching your docs or anything else.\n\n## The Emergence of ‚ÄúLLMs.txt‚Äù and Other Standards\n\nPeople keep floating new proposals like ‚ÄúLLMs.txt‚Äù (for providing an AI-friendly index of your docs) or more formal protocols like Anthropic‚Äôs Model Context Protocol. Regardless of which approaches stick, the takeaway is simple: we‚Äôre no longer just speaking to human devs. We‚Äôre also speaking to software that tries to parse our docs and hit our endpoints automatically. If we don‚Äôt make that easy‚Äîthrough well-defined endpoints, frictionless signup, and extensions for AI tools‚Äîwe‚Äôre invisible to the agent.\n\n## Conclusion: A Shift in Mindset\n\nAdapting to an **AI Go-To-Market** strategy doesn‚Äôt mean ditching your docs or ignoring developers. It means adding another layer‚Äî‚Äúagentic‚Äù integrations that allow popular AI tools to actually interact with your API in a way that makes sense. That might be a GitHub Copilot extension, a specialized plugin for ChatGPT, or a standard file like ‚Äúllms.txt.‚Äù Once you‚Äôre agent-friendly, you become part of the new funnel developers are increasingly relying on.\n\nI‚Äôve seen personally just how powerful this can be. Companies that invest in these agentic workflows see devs adopt their product faster, often skipping the ‚Äúhuman read the docs‚Äù stage entirely. In many ways, AI tools are becoming the new ‚Äúworkspace‚Äù for developers‚Äîif your product isn‚Äôt easily discoverable or usable from inside these AI-driven environments, you won‚Äôt appear in their pipeline. It‚Äôs not a matter of hype; it‚Äôs already happening. If your platform isn‚Äôt ready for the next wave of AI-centric adoption, you risk losing out on devs who rely on AI agents for most of their coding and decision-making. **AI is a new distribution channel**‚Äîtreat it as such, stay ahead of the curve, and see usage and revenue grow."},{"id":"layer_neon_copilot_extension","metadata":{"permalink":"/layer_neon_copilot_extension","source":"@site/blog/layer_neon_copilot_extension.mdx","title":"How Layer Built Neon‚Äôs Copilot Extension","description":"This blog demonstrates how Layer created a GitHub Copilot extension for Neon, a serverless, open-source PostgreSQL database. We build Copilot extensions (and other AI integrations) for companies that want to embrace AI Agents in their go-to-market process. If you‚Äôd like to see how we can help, read on‚Äîor book a demo to learn more","date":"2025-02-20T00:00:00.000Z","tags":[{"inline":true,"label":"LLM Extensibility","permalink":"/tags/llm-extensibility"},{"inline":true,"label":"API","permalink":"/tags/api"},{"inline":true,"label":"Developer Experience (DX)","permalink":"/tags/developer-experience-dx"},{"inline":true,"label":"Agent Experience (AX)","permalink":"/tags/agent-experience-ax"},{"inline":true,"label":"GitHub Copilot","permalink":"/tags/git-hub-copilot"},{"inline":true,"label":"Neon","permalink":"/tags/neon"}],"readingTime":2.67,"hasTruncateMarker":false,"authors":[{"name":"Andrew Hamilton","title":"Co-Founder and CTO @ Layer","url":"https://github.com/andrewlayer","imageURL":"https://avatars.githubusercontent.com/u/135887157?v=4","key":"aham","page":null}],"frontMatter":{"id":"layer_neon_copilot_extension","slug":"layer_neon_copilot_extension","title":"How Layer Built Neon‚Äôs Copilot Extension","authors":["aham"],"date":"2025-02-20T00:00:00.000Z","description":"This blog demonstrates how Layer created a GitHub Copilot extension for Neon, a serverless, open-source PostgreSQL database. We build Copilot extensions (and other AI integrations) for companies that want to embrace AI Agents in their go-to-market process. If you‚Äôd like to see how we can help, read on‚Äîor book a demo to learn more","tags":["LLM Extensibility","API","Developer Experience (DX)","Agent Experience (AX)","GitHub Copilot","Neon"],"keywords":["LLM Extensibility","API","Developer Experience (DX)","Agent Experience (AX)","GitHub Copilot","Neon"]},"unlisted":false,"prevItem":{"title":"AI Go-To-Market: Why Agents Are Your Newest Path to Adoption","permalink":"/ai_gtm"},"nextItem":{"title":"Why LLM Extensibility is Vital to API Vendors","permalink":"/llm_extensibility_vital_to_api_vendors"}},"content":"<head>\n  <title>Layer AI Blog | How Layer Built Neon‚Äôs Copilot Extension</title>\n  <meta property=\"og:title\" content=\"Layer AI Blog | How Layer Built Neon‚Äôs Copilot Extension\"></meta>\n</head>\n\n_The following blog demonstrates how Layer created a GitHub Copilot extension for [Neon](https://neon.tech/), a serverless, open-source PostgreSQL database. We build Copilot extensions (and other AI integrations) for companies that want to embrace AI Agents in their go-to-market process. If you‚Äôd like to see how we can help, read on‚Äîor [book a demo](https://cal.com/team/layer/demo) to learn more._\n\n## Turning your docs into AI integrations\n\nAs soon as GitHub Copilot burst onto the scene, teams started asking, ‚ÄúHow do we teach Copilot about our platform?‚Äù Whether you‚Äôre offering a specialized set of APIs or, in Neon‚Äôs case, a managed Postgres service, developers increasingly want AI-driven integrations that bring your endpoints and best practices directly into their coding workflow. In other words, it‚Äôs no longer enough to rely on devs hunting through documentation and using products through the GUI.\n\nThat‚Äôs exactly where Layer fits in. \n\nWe help companies build Copilot extensions‚Äîwritten in any language‚Äîthat seamlessly inject your product‚Äôs logic into GitHub Copilot (and other AI surfaces, too, including Cursor, VS Code, ChatGPT, and more). Here are the steps we took to build **Neon‚Äôs** Copilot Extension‚Äîexplaining why a ‚ÄúCopilot extension‚Äù isn‚Äôt always what you might expect.\n\n## Prerequisites: Python, Ngrok\n\nFirst, make sure that you have the latest versions of [Python](https://www.python.org/downloads/) and [Ngrok](https://ngrok.com/downloads/) installed onto your system.\n\n## Webhook Subscriptions: Like Building a Discord Bot\n\nIf you‚Äôve built a Discord bot before, you‚Äôre already familiar with this pattern: you set up a server, then point the platform at it so it can deliver events to your endpoint. That‚Äôs exactly how GitHub Copilot extensions work under the hood: You create a web server‚Äîwritten in any language you like‚Äîand subscribe it to events from GitHub. Whenever Copilot needs to query your extension, it fires an HTTP request to your server.\n\nThis ‚Äúsubscription‚Äù approach is powerful for two reasons:\n1. **Language freedom.** Because all you need is a server that handles HTTP requests, you‚Äôre not locked into Python, Java, Node, or any specific tech stack. If you can spin up a server, you can handle Copilot extension calls‚Äîwhether you prefer Go, OCaml, or even Ruby.\n2. **Unlimited custom logic.** Once the requests arrive, it‚Äôs entirely up to you how to process them. Want to authenticate users, pull data from a Postgres database, or call a third-party API? Go for it. The webhook subscription doesn‚Äôt dictate how your code runs; it just ensures Copilot knows where to send requests.\n\nIn other words, the only thing you really need to do is let GitHub know where your server lives. Once that‚Äôs done, you can implement your Copilot ‚Äúextension‚Äù in the language of your choice, and handle incoming Copilot requests in whatever way best suits your application‚Äôs needs.\n\n![Copilot Extension Workflow](./assets/layer_neon_copilot_extension/copilot_extension.jpg)\n\n## Our Server\n\nBelow is the starter code we used at **Layer** to help Neon‚Äôs team stand up their Copilot server. It accomplishes two main objectives:\n1. **Expose** a /completion route for Copilot to hit via POST.\n2. **Craft** a system message injected into the conversation to modify the AI‚Äôs response.\n\nCLICK [HERE](https://neon.tech/blog/how-to-build-github-copilot-extensions) TO READ THE REST OF THE BLOG."},{"id":"llm_extensibility_vital_to_api_vendors","metadata":{"permalink":"/llm_extensibility_vital_to_api_vendors","source":"@site/blog/llm_extensibility_vital_to_api_vendors.mdx","title":"Why LLM Extensibility is Vital to API Vendors","description":"Layer AI Blog | Why LLM Extensibility is Vital to API Vendors","date":"2025-02-13T00:00:00.000Z","tags":[{"inline":true,"label":"LLM Extensibility","permalink":"/tags/llm-extensibility"},{"inline":true,"label":"API","permalink":"/tags/api"},{"inline":true,"label":"Developer Experience (DX)","permalink":"/tags/developer-experience-dx"},{"inline":true,"label":"Agent Experience (AX)","permalink":"/tags/agent-experience-ax"}],"readingTime":8.405,"hasTruncateMarker":false,"authors":[{"name":"Jonah Katz","title":"Co-Founder and CEO @ Layer","url":"https://www.linkedin.com/in/jonahkatz1/","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQH2LNM38t56gw/profile-displayphoto-shrink_400_400/B4DZUFQFoiHwAg-/0/1739549859357?e=1745452800&v=beta&t=_d4NnBv-PKurwk7BTGq6qFBoHQhwvZzBS9_zagNjpeE","key":"jkatz","page":null},{"name":"Andrew Hamilton","title":"Co-Founder and CTO @ Layer","url":"https://github.com/andrewlayer","imageURL":"https://avatars.githubusercontent.com/u/135887157?v=4","key":"aham","page":null}],"frontMatter":{"id":"llm_extensibility_vital_to_api_vendors","slug":"llm_extensibility_vital_to_api_vendors","title":"Why LLM Extensibility is Vital to API Vendors","authors":["jkatz","aham"],"date":"2025-02-13T00:00:00.000Z","descripion":"Learn why it's important for API vendors to invest in an LLM Extensibility strategy to ensure their products stay relevant and accessible in a post-AI-Agent era.","tags":["LLM Extensibility","API","Developer Experience (DX)","Agent Experience (AX)"],"keywords":["LLM Extensibility","API","Developer Experience (DX)","Agent Experience (AX)"]},"unlisted":false,"prevItem":{"title":"How Layer Built Neon‚Äôs Copilot Extension","permalink":"/layer_neon_copilot_extension"},"nextItem":{"title":"LLM Extensibility 101","permalink":"/llm_extensibility"}},"content":"<head>\n  <title>Layer AI Blog | Why LLM Extensibility is Vital to API Vendors</title>\n  <meta property=\"og:title\" content=\"Layer AI Blog | Why LLM Extensibility is Vital to API Vendors\"></meta>\n  <meta property=\"og:image\" content=\"https://storage.googleapis.com/generic-assets/blog-static-assets/llm_extensibility_vital_og_image.png\"></meta>\n</head>\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\nimport {TwitterTweetEmbed} from 'react-twitter-embed';\n\nAI usage has exploded in the past few years, with Large Language Model (LLM) based tools like ChatGPT, Cursor, Lovable, and GitHub Copilot weaving their way into developers‚Äô daily workflows. It‚Äôs not just ‚Äúchatbots‚Äù anymore‚Äîthese models are now capable of agentic behavior, meaning they can execute code, connect to external services, and perform tasks automatically. This shift is profound, and it‚Äôs already impacting how we build and integrate APIs. However, many API vendors haven‚Äôt yet considered what happens when these AI ‚Äúagents‚Äù start calling their endpoints.\n\nDo you really know how your API is being used‚Äîor misused‚Äîby intelligent systems? And just as important, how can you steer these interactions so that they‚Äôre actually helpful for developers and customers?\n\nThat‚Äôs where [LLM Extensibility](https://docs.buildwithlayer.com/llm_extensibility) comes in. It‚Äôs a new strategy to ensure agentic AI tools can actually understand and use your APIs. As a result, everyone wins‚Äîdevelopers get seamless AI-driven integrations, brands see their APIs used correctly and securely, and AI tools deliver more accurate, trusted results.\n\n## From Chat Windows to Agentic AI\n\nUntil recently, most people pictured LLMs as glorified text boxes‚Äîoffer a question, receive a plausible-sounding answer. But anyone following major AI developer updates sees the real trend: LLMs are evolving into agents. They don‚Äôt just suggest code; they can run it, authenticate to services, create pull requests, and even manage entire workflows. For API vendors, this is both an opportunity and a looming challenge. On one hand, imagine a developer who needs to spin up a new database or update a payment link. They could simply type a command into GitHub Copilot (or any AI workspace of choice) and instantly connect to your API, without toggling between multiple dashboards‚Äîan incredibly smooth experience if your API is ‚ÄúAI-ready.‚Äù On the other hand, AI models sometimes hallucinate, guessing at non-existent endpoints or interpreting parameters incorrectly if they rely on outdated or partial docs. Without a standardized, brand-approved method of calling your APIs, you risk a flurry of frustrated devs and broken requests you never even knew about.\n\n## LLM Extensibility: The Unified Solution\n\nLLM Extensibility is a strategy for making your API ‚Äúfriendly‚Äù to emerging AI agents. Rather than letting models haphazardly scrape your docs, you provide a curated, up-to-date extension that tells AI tools exactly how your endpoints function and how to call them. The approach typically involves:\n\n- **Publishing** official endpoints and usage instructions (often derived from OpenAPI specs or another structured format).\n- **Declaring** which AI surfaces or agentic tools can access which endpoints, and how.\n- **Maintaining** versioning and updates in real time, so the AI always ‚Äúknows‚Äù the latest shape of your API.\n\nYou‚Äôre essentially turning ‚ÄúRead my docs and guess what to do‚Äù into ‚ÄúHere‚Äôs exactly how to perform that action,‚Äù bridging the gap between AI guesswork and real expertise with your product. It‚Äôs a controlled handoff of knowledge that ensures AI agents behave like experts‚Äînot clueless interns pressing random buttons.\n\n## AX: Treating AI Agents Like a New Persona\n\nSome industry leaders, like Mathias Biilmann, co-founder of Netlify, talk about [\"Agent Experience (AX)\"](https://biilmann.blog/articles/introducing-ax/)‚Äîurging platforms to design for AI agents as a core user persona. If you expose your API to a variety of surfaces‚Äîrather than locking it into a single proprietary platform‚Äîyou enable devs (and AI agents) to discover, sign up for, and use your product without friction. That‚Äôs effectively the same vision LLM Extensibility pursues: an open, ‚Äúagent-friendly‚Äù ecosystem where your API is easy for any LLM or agent to harness.\n\n<TwitterTweetEmbed tweetId=\"1884285755738177591\" />\n\n## ‚ÄúWhy Not Just Let ChatGPT Scrape My Docs?‚Äù\n\nIt‚Äôs true that modern LLMs can browse the web or parse PDFs. But that approach is inherently unreliable for complex APIs. Scraping can mix up old and new references, miss authentication steps, or fail to reflect your latest endpoints. Worse, you never see the queries developers are typing into AI tools about your product, which makes it difficult to understand their pain points and improve DX.\n\nWhen it comes to AI, precision, security, and brand consistency are all major concerns. If the AI is making up endpoints or exposing them incorrectly, you could end up with errors, breaches, or a damaged reputation. According to [OpenAI‚Äôs Actions Introduction](https://platform.openai.com/docs/actions/introduction), controlling the scope and validity of AI calls is crucial for ensuring safety and correctness. By exposing your API to AI agents  through LLM extensions, you embed the logic, security parameters, and approved language right into the AI environment‚Äîa structured handshake rather than a free-for-all.\n\n## AI Agents as the New Interface\n\nWe‚Äôre used to spinning up a dashboard or CLI when we need to create a database, trigger a payment, or update user data. But the future points to AI as a command center for handling multiple APIs in one interface. Instead of visiting Stripe or Plaid‚Äôs website, you might simply say, ‚ÄúCreate a new subscription tier for User ID #123,‚Äù and let the AI Agent take care of the details. This unification is no longer science fiction. GitHub Copilot offers chat-based coding assistance integrated into your IDE, while startups like Bolt and Lovable are building agentic platforms that can run code, call APIs, and orchestrate tasks on your behalf.\n\nStill, the question remains:\n\n> ‚ÄúIs your API primed for such interactions‚Äîor stuck in a pre-agent era?‚Äù\n\nSome people argue that Retrieval-Augmented Generation (RAG) alone could replace structured integrations. RAG is powerful for reading and summarizing text, but as AI becomes more autonomous‚Äîand can issue refunds or provision servers‚Äîpure text retrieval is no longer enough. You need a pipeline that handles authentication, updates, and precise parameter calls in real time.\n\n## The Real Power: Control and Analytics\n\nLLM Extensibility isn‚Äôt just about preventing hallucinations; it‚Äôs also about knowing exactly what developers (and AI agents) are doing. When queries route through a single extension, you can see which endpoints people are calling, which questions they repeatedly ask, and where confusion arises. That visibility is gold. Maybe you find that hundreds of devs keep stumbling to set up partial refunds in your payments API, suggesting you need clearer docs or a simpler endpoint. Or perhaps users are asking about the next 10x feature that doesn‚Äôt quite exist yet. At the end of the day, the exact queries users are typing into tools like ChatGPT and Copilot when it comes to your product are incredibly valuable data about the pain points of your users and what they care about. And without an extensibility strategy, the only parties on the receiving end of this data are the model makers (OpenAI, GitHub, etc).\n\n<Tabs>\n  <TabItem value=\"standard\" label=\"Standard LLM Experience\" default>\n   <div style={{\"text-align\": \"center\"}}>\n        ![Standard LLM Experience](./assets/llm_extensibility_vital_to_api_vendors/standard_llm_experience.png)\n    </div>\n  </TabItem>\n  <TabItem value=\"extensibility\" label=\"LLM Extensibility Experience\" >\n    <div style={{\"text-align\": \"center\"}}>\n        ![LLM Extensibility Experience](./assets/llm_extensibility_vital_to_api_vendors/llm_extensibility_experience.png)\n    </div>\n  </TabItem>\n</Tabs>\n\n## The Emergence of an AI App Store\n\nThink back to the iPhone‚Äôs earliest days: powerful hardware, but limited until the App Store gave developers the ability to build on top of it. Today‚Äôs LLMs are similarly powerful, but need a standardized way to access and use real-world APIs. That‚Äôs what LLM Extensibility provides. By building a universal extension for your API‚Äîcomplete with authentication rules, usage scenarios, and brand guidelines‚Äîyou give agentic AI surfaces a roadmap to your service. Instead of building one plugin for ChatGPT, another for Claude, and yet another for Copilot, you can unify your docs and API endpoints in one LLM extension. That way, devs (and their AI assistants) can discover your functionality wherever they work‚Äîwithout you having to manage multiple integrations. This might be the dawn of an ‚ÄúAI App Store,‚Äù where complex functionality ranging from payment processing to database management sits behind carefully crafted LLM extensions.\n\n<Tabs>\n  <TabItem value=\"independent\" label=\"Independent LLM Extensions\" default>\n   <div style={{\"text-align\": \"center\"}}>\n        ![Independent LLM Extensions](./assets/llm_extensibility/fragmented_distribution_model.png)\n    </div>\n  </TabItem>\n  <TabItem value=\"unified\" label=\"Unified LLM Extension\" >\n    <div style={{\"text-align\": \"center\"}}>\n        ![Unified LLM Extension](./assets/llm_extensibility/unified_distribution_model.png)\n    </div>\n  </TabItem>\n</Tabs>\n\n## Why Now? Why Vendors Must Lean In\n\nFor years, we‚Äôve managed APIs through traditional docs and developer portals. But as AI becomes more active in software creation and ops, those portals risk irrelevance if they can‚Äôt speak to agentic LLMs. The next generation of developers are sitting in classrooms right now, already using ChatGPT for homework. We may never see them flipping through Stripe dashboards‚Äîthese future devs will simply talk to an AI that can spin up or tear down resources in seconds, if your API is ready for it.\n\nLLM Extensibility ensures you‚Äôre not sidelined in this shift. You define how your API is exposed, which endpoints are valid, and how your brand is presented. The payoff? More customers and more revenue. Developers can seamlessly sign up for, onboard to, and use your APIs and SDKs directly from the AI tools they know and love.\n\nIt‚Äôs all about future-proofing your API in a world where ‚Äúchat‚Äù is just the beginning, and agentic AI is the new interface. Some vendors will build extensions one by one with each ecosystem, while others will use platforms like [Layer](https://buildwithlayer.com) to manage deployments to multiple AI surfaces at once. Either way, API vendors investing in LLM Extensibility now have a major edge. When new customers can seamlessly call your service from any AI environment, growth accelerates, engagement deepens, and your brand stands out as a genuine innovator. It‚Äôs not just about better docs or fewer hallucinations‚Äîit‚Äôs about thriving in an era where AI is your newest‚Äîand most demanding‚Äîuser.\n\n## Conclusion: The Next Era of API Integration\n\nWe‚Äôre entering an era where the best API experiences aren‚Äôt defined solely by ‚Äòbeautiful docs‚Äô but by how easily AI agents can consume them. For companies like [Neon](https://neon.tech/blog/how-to-build-github-copilot-extensions) who are already investing in LLM Extensibility, **the real aim is to avoid becoming invisible as more production code is written by AI agents**‚Äînot just humans. That‚Äôs why API vendors must ensure their products fit seamlessly into an AI-driven world, where every service is a single command away."},{"id":"llm_extensibility","metadata":{"permalink":"/llm_extensibility","source":"@site/blog/llm_extensibility.mdx","title":"LLM Extensibility 101","description":"Layer AI Blog | LLM Extensibility 101","date":"2025-01-28T00:00:00.000Z","tags":[{"inline":true,"label":"LLM Extensibility","permalink":"/tags/llm-extensibility"}],"readingTime":6.12,"hasTruncateMarker":false,"authors":[{"name":"Jonah Katz","title":"Co-Founder and CEO @ Layer","url":"https://www.linkedin.com/in/jonahkatz1/","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQH2LNM38t56gw/profile-displayphoto-shrink_400_400/B4DZUFQFoiHwAg-/0/1739549859357?e=1745452800&v=beta&t=_d4NnBv-PKurwk7BTGq6qFBoHQhwvZzBS9_zagNjpeE","key":"jkatz","page":null},{"name":"Andrew Hamilton","title":"Co-Founder and CTO @ Layer","url":"https://github.com/andrewlayer","imageURL":"https://avatars.githubusercontent.com/u/135887157?v=4","key":"aham","page":null}],"frontMatter":{"id":"llm_extensibility","slug":"llm_extensibility","title":"LLM Extensibility 101","authors":["jkatz","aham"],"date":"2025-01-28T00:00:00.000Z","descripion":"What is LLM Extensibility? Learn how devs, brands, and AI surfaces unify their workflows for real-time, reliable AI‚Äîwithout fragmented code or docs.","tags":["LLM Extensibility"],"keywords":["LLM Extensibility"]},"unlisted":false,"prevItem":{"title":"Why LLM Extensibility is Vital to API Vendors","permalink":"/llm_extensibility_vital_to_api_vendors"},"nextItem":{"title":"Keep Your Data DRY with APIFlask","permalink":"/dry_with_api_flask"}},"content":"<head>\n  <title>Layer AI Blog | LLM Extensibility 101</title>\n  <meta property=\"og:title\" content=\"Layer AI Blog | LLM Extensibility 101\"></meta>\n  <meta property=\"og:image\" content=\"https://storage.googleapis.com/generic-assets/blog-static-assets/llm_extensibility_og_image.png\"></meta>\n</head>\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n## The Trust Gap in AI-Assisted Coding\n\nAsk any developer who‚Äôs turned to an AI chatbot for coding help, and you‚Äôll likely hear the same question, reverberating at various volumes in their heads: *‚ÄúCan I really trust this snippet?‚Äù*\n\nYou never know if the chatbot is scraping outdated docs, merging conflicting tutorials, or pulling random code from unmaintained repos. Worse, you‚Äôre forced to juggle multiple platforms‚ÄîIDE, browser, chat window‚Äîjust to test and integrate those suggestions.\n\nNobody wants to waste hours verifying code that was supposed to save them time. That‚Äôs where LLM Extensibility steps in, bringing curated, brand-approved-and-controlled data that stays in sync across every AI environment you rely on. The result? A coding assistant you can actually trust, with fewer late-night debugging sessions and far less copy-paste chaos.\n\n<Tabs>\n  <TabItem value=\"standard\" label=\"Standard LLM Coding Assistance\" default>\n   <div style={{\"text-align\": \"center\"}}>\n        ![Standard LLM Coding Assistance](./assets/llm_extensibility/standard_llm_coding_assistance.png)\n    </div>\n  </TabItem>\n  <TabItem value=\"extensibility\" label=\"LLM Extension Coding Assistance\" >\n    <div style={{\"text-align\": \"center\"}}>\n        ![LLM Extension Coding Assistance](./assets/llm_extensibility/llm_extension_coding_assistance.png)\n    </div>\n  </TabItem>\n</Tabs>\n\n## What Is LLM Extensibility‚Äîand Why Does It Matter?\n\n**LLM Extensibility** is the process of embedding brand-approved, continuously updated knowledge and functionality directly into AI environments‚Äîso that developers, businesses, and AI platforms can collaborate more effectively. Instead of letting tools like ChatGPT or GitHub Copilot scrape the open web for partial information, LLM Extensibility ensures they always draw from the most reliable, real-time sources.\n\nThis means a single, consistent pipeline: when a company publishes official documentation, code samples, or best practices into an ‚ÄúLLM extension,‚Äù developers see the correct info wherever they use AI‚Äîwhether that‚Äôs in VS Code, Copilot, ChatGPT, Anthropic, or any other AI surface. No guesswork, no stale references, and a drastically simpler way to trust the AI‚Äôs output.\n\n![What is LLM Extensibility?](./assets/llm_extensibility/what_is_llm_extensibility.png)\n\n## Everybody Wins: Unifying Brands, Developers, and AI Surfaces\n\nWhen your API is constantly evolving (think Stripe, Plaid, Neon, or any fast-moving platform), it‚Äôs no small feat to keep docs consistent across various AI tools. Developers often piece together partially-hallucinated code snippets with trial and error, leading to bugs, confusion, and extra support overhead. LLM Extensibility solves that by offering a single source of truth for docs, code samples, and best practices‚Äîacross every AI environment that matters. \n1. **Brands** gain direct control over how popular AI tools assist developers with their APIs and SDKs. By building extensions for ecosystems like ChatGPT, Claude, and GitHub Copilot, they embed up-to-date, brand-approved content and functionality right where developers live‚Äîensuring consistency, reliability, and a smoother path to integration.\n2. **Developers** can trust that what they‚Äôre pulling is always current and aligned with official best practices. Plus, the AI can do more than advise: it can open pull requests, run tests, or spin up resources in the developer‚Äôs environment, guided by reliable content.\n3. **AI Surfaces** (VS Code, ChatGPT, GitHub Copilot, Anthropic, etc.) deliver a richer, more consistent user experience. Instead of scraping partial data and generating hallucinated code, they plug into a universal source of truth that updates in real time and brings powerful agentic functionality to their users.\n\nOf course, not every AI platform has embraced extensibility yet. Some, like Cursor, remain closed off‚Äîfor now. But as the [ecosystem around LLM extensibility grows](https://publish.obsidian.md/andrew-vault/Layer/Blogs/LLM+Extensibility-+Race+for+an+Ecosystem), user expectations will evolve, and every surface will likely open up to brand-specific extensions to stay competitive.\n\n## Why Not Just ‚ÄúAsk ChatGPT‚Äù?\n\nIf ChatGPT (or another super powerful LLM) can browse the web, why bother with a brand-managed extension? The short answer is **reliability and control.**\n\nOpen-web crawling might yield half-correct references or piecemeal code that no longer matches a brand‚Äôs latest version. An official extension, on the other hand, feeds the AI precisely what the brand wants it to see‚Äînothing more, nothing less.\n\nFor developers, that means fewer misfires. Instead of sifting through questionable snippets, they can rely on curated, brand-sanctioned responses. Couple that with the AI‚Äôs ability to manipulate files in your IDE or open pull requests in GitHub, and you‚Äôve got an active collaborator rather than a passive advisor. You still oversee or approve changes, but the manual copy-paste grind disappears.\n\n## One Extension, Many Surfaces‚ÄîBuild Once, Publish Everywhere\n\nYes, a company could build separate plugins for each AI environment‚Äî[VS Code extension](https://code.visualstudio.com/api/extension-guides/chat), [Github Copilot extension](https://github.com/marketplace?type=apps&copilot_app=true), [OpenAI GPT](https://chatgpt.com/gpts), [Anthropic MCP Server](https://github.com/modelcontextprotocol/servers)‚Äîand manually update each one whenever their docs or OpenAPI spec changes. But that approach is time-consuming and prone to version drift.\n\nA **single publishing model** makes far more sense: create one extension (containing your official docs, code samples, or agentic actions), then **deploy** it to whichever AI surfaces you want to support. Whenever you update your docs or add new features, every environment reflects the change at once. It‚Äôs akin to how React Native lets you build an app once and distribute it to multiple platforms (iOS, Android, MacOS, Windows, and Web); here, you‚Äôre uploading and maintaining **brand** content and logic on one platform, and distributing to multiple AI tools with no code required.\n\nThis is the vision behind **platforms like Layer**, which aim to unify LLM Extensibility. Rather than building piecemeal integrations for each environment, you create a single extension‚Äîyour ‚Äúsource of truth‚Äù‚Äîand publish it across supported surfaces from one central dashboard. Brands update content once, developers find consistent, up-to-date docs and agentic tools, and AI surfaces reap the benefits of official, high-quality knowledge.\n\n<Tabs>\n  <TabItem value=\"fragmented\" label=\"Fragmented Distribution Model\" default>\n   <div style={{\"text-align\": \"center\"}}>\n        ![Fragmented Distribution Model](./assets/llm_extensibility/fragmented_distribution_model.png)\n    </div>\n  </TabItem>\n  <TabItem value=\"unified\" label=\"Unified Distribution Model\" >\n    <div style={{\"text-align\": \"center\"}}>\n        ![Unified Distribution Model](./assets/llm_extensibility/unified_distribution_model.png)\n    </div>\n  </TabItem>\n</Tabs>\n\n## Insights and Efficiency for Everyone\n\nOne huge benefit of LLM Extensibility is the **insight** it provides. Brands that control their extension can track real-world usage: which queries come up most often, which features confuse developers, and where the biggest gaps lie. That feedback loop shapes future documentation tweaks and even core product decisions.\n\nDevelopers, meanwhile, can streamline their workflows. They no longer hunt down the right doc version or wonder if a snippet is still valid; the AI always references the latest info. And AI surfaces gain a reputation for **trustworthy** guidance, pulling from an official source rather than stitching together random web scraps.\n\n## Where AI Goes from Here\n\nWe‚Äôre already seeing signs that AI can do more than suggest code‚Äîit can act. Opening pull requests, provisioning services, or orchestrating CI/CD pipelines are all becoming part of an LLM‚Äôs repertoire. **LLM Extensibility** paves the way for that evolution by grounding these actions in brand-approved data and logic. And as more AI surfaces become extensible, the line between ‚ÄúAI advice‚Äù and ‚ÄúAI-driven automation‚Äù continues to blur.\n\nThat‚Äôs good news for everyone in this conversation: brands, developers, and AI platform providers. With a unified extensibility model, changes happen once, code is consistently accurate, and developers can do more with less friction. Instead of scraping questionable snippets or juggling plugin updates, the future looks a lot more **connected**‚Äîand a lot more **trustworthy.**\n\nThat‚Äôs the essence of **LLM Extensibility:** a blueprint for AI that respects brand control, fosters developer confidence, and unlocks richer, continuously updated capabilities across all the surfaces where work actually happens. If you‚Äôre ready to leave behind scattered docs and fragmented plugin strategies, this could be the next big step toward a smarter, more seamless AI pipeline."},{"id":"dry_with_api_flask","metadata":{"permalink":"/dry_with_api_flask","source":"@site/blog/dry_with_api_flask.mdx","title":"Keep Your Data DRY with APIFlask","description":"Layer AI Blog | Keep Your Data DRY with APIFlask","date":"2024-12-20T00:00:00.000Z","tags":[{"inline":true,"label":"DRY","permalink":"/tags/dry"},{"inline":true,"label":"APIFlask","permalink":"/tags/api-flask"},{"inline":true,"label":"SQLAlchemy","permalink":"/tags/sql-alchemy"},{"inline":true,"label":"Marshmallow","permalink":"/tags/marshmallow"},{"inline":true,"label":"OneOfSchema","permalink":"/tags/one-of-schema"}],"readingTime":12.64,"hasTruncateMarker":false,"authors":[{"name":"Gavyn Partlow","title":"Software Engineer @ Layer","url":"https://github.com/GavynWithLayer","imageURL":"https://avatars.githubusercontent.com/u/151399782?v=4","key":"gpart","page":null},{"name":"Lucas Gismondi","title":"Software Engineer @ Layer","url":"https://github.com/lucasgismondi-gg","imageURL":"https://avatars.githubusercontent.com/u/121511329?v=4","key":"lgis","page":null},{"name":"Andrew Hamilton","title":"Co-Founder and CTO @ Layer","url":"https://github.com/andrewlayer","imageURL":"https://avatars.githubusercontent.com/u/135887157?v=4","key":"aham","page":null}],"frontMatter":{"id":"dry_with_api_flask","slug":"dry_with_api_flask","title":"Keep Your Data DRY with APIFlask","authors":["gpart","lgis","aham"],"date":"2024-12-20T00:00:00.000Z","descripion":"Lead by Gavyn, Co-authored by Lucas and Andrew","tags":["DRY","APIFlask","SQLAlchemy","Marshmallow","OneOfSchema"]},"unlisted":false,"prevItem":{"title":"LLM Extensibility 101","permalink":"/llm_extensibility"}},"content":"<head>\n  <title>Layer AI Blog | Keep Your Data DRY with APIFlask</title>\n  <meta name=\"title\" property=\"og:title\" content=\"Layer AI Blog | Keep Your Data DRY with APIFlask\"></meta>\n</head>\n\n<small>\n**Lead Author:** Gavyn<br/>\n**Co-Authors:** Lucas, Andrew<br/>\n\nAssociated Repository: [blog-dry_api_flask_demo](https://github.com/buildwithlayer/blog-dry_api_flask_demo)<br/>\n</small>\n\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n## ‚òîÔ∏è When it starts to rain \nWhen working with a traditional [Model/View/Controller](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller) approach, \nit is easy to fall suspect to code duplication. I've\nseen it with coworkers, friends, and even family. No one is safe from code duplication. However, there are some tips and\ntricks you can use with Flask to help protect yourself and your loved ones.\n\n## Data Sources\nFirst, let's talk about where data comes from and how it can trick us into making the same models multiple times.\n\n### Database + SQLAlchemy\n\nThe main source of data in most backends is a database (it's in the name). If you've been around the block and done more\nthan a few python APIs, you're probably familiar with tools like [Flask](https://flask.palletsprojects.com/en/) and [SQLAlchemy](https://www.sqlalchemy.org/). SQLAlchemy is great to help you\nmodel and manage data in your database without ever writing a line of SQL, and that's every developer's dream.\n\nWhen working with Flask and SQLAlchemy, you'll often see ORM models like this:\n\n\n```python\nclass Farmer(db.Model):\n    id: Mapped[int] = mapped_column(\n        Integer, primary_key=True,\n    )\n    created_at: Mapped[DateTime] = mapped_column(\n        DateTime, nullable=False, server_default=func.now(),\n    )\n    updated_at: Mapped[DateTime] = mapped_column(\n        DateTime, nullable=False, server_default=func.now(), onupdate=func.now(),\n    )\n    name: Mapped[str] = mapped_column(\n        String, nullable=False,\n    )\n```\n\nAnd this is great! You've got an abstraction of the columns in your `farmer` table. Not only can you read, create,\nupdate, and delete your farmers from your database with ease, but you can also make changes to the table itself, and\nSQLAlchemy will help you migrate your data. Very developer friendly and very useful!\n\n### APIs + Marshmallow\n\nThe next source of data in any API backend is the APIs themselves! You've got two categories of data: requests and\nresponses. In many cases, developers follow a model/view/controller pattern, and the GET routes are returning something\nnearly identical to the ORM model.\n\nLet's extend our example:\n\n```python\nfarmers_bp = APIBlueprint(\n    \"farmers\", __name__, enable_openapi=True\n)\n\n# Marshmallow Schema\nclass FarmerOut(Schema):\n    id = fields.Integer(required=True)\n    created_at = fields.DateTime(required=True)\n    updated_at = fields.DateTime(required=True)\n    name = fields.String(required=True)\n\n# Flask Route\n@farmers_bp.get(\"/<int:farmer_id>\")\n@farmers_bp.output(FarmerOut)\ndef get_farmer_by_id(farmer_id: int):\n    farmer = Farmer.query.where(Farmer.id == farmer_id).first()\n    if farmer is None:\n        raise HTTPError(404, message=\"Farmer not found\")\n    return farmer\n```\n\nNow if there exists a record in our database, we can ping `farmers/1` and get the following response:\n\n```js\n{\n  \"created_at\": \"2023-12-12T15:51:00\",\n  \"id\": 1,\n  \"name\": \"Old MacDonald\",\n  \"updated_at\": \"2023-12-12T15:51:00\"\n}\n```\n\n### üåä Monsoon Season \n\nThe well-seasoned developer might dust off their salt and pepper and say, \"Wait! I've seen those same fields before!\"\nAnd they'd be right! Looking at the `Farmer` class and the `FarmerOut` class, the fields are nearly identical.\n\n\n<Tabs>\n  <TabItem value=\"orange\" label=\"SQLAlchemy\" default>\n    \n    ```python\n    # SQLAlchemy Schema\n    class Farmer(db.Model):\n        id: Mapped[int] = mapped_column(Integer, primary_key=True)\n        created_at: Mapped[DateTime] = mapped_column(DateTime, nullable=False, server_default=func.now())\n        updated_at: Mapped[DateTime] = mapped_column(DateTime, nullable=False, server_default=func.now(), onupdate=func.now())\n        name: Mapped[str] = mapped_column(String, nullable=False)\n    ```\n  </TabItem>\n  <TabItem value=\"apple\" label=\"Marshmallow\" >\n    ```python\n    # Marshmallow Schema\n    class FarmerOut(Schema):\n        id = fields.Integer(required=True)\n        created_at = fields.DateTime(required=True)\n        updated_at = fields.DateTime(required=True)\n        name = fields.String(required=True)\n    ```\n  </TabItem>\n</Tabs>\n\nThis is definitely a bad look. Imagine if we were to add a new field to the `Farmer` class? Or even more sneaky, change\nthe type of one of the fields? We'd then have to update `FarmerOut` and any other schemas we may have in the future that\ninclude `Farmer` to match. This is a burden on developers, but it also is a chance for subtle bugs to creep in.\n\n## Buy 1, Get 1 Free!\n\nThankfully, we have some tools at our disposal to help avoid this kind of disaster. Enter [`SQLAlchemyAutoSchema`](https://marshmallow-sqlalchemy.readthedocs.io/en/latest/api_reference.html), stage\nleft. Let's look at how we can use [`flask-marshmallow`](https://flask-marshmallow.readthedocs.io/en/latest/) and `SQLAlchemyAutoSchema` to help avoid all this duplication.\n\n### Simple Example\n\nBelow our `Farmer` definition, we can add a new class for the `FarmerSchema` as follows:\n\n```python\nclass FarmerSchema(marsh.SQLAlchemyAutoSchema):\n    class Meta:\n        model = Farmer\n```\n\nThen, we just update our route to use this new schema:\n\n```python\n@farmers_bp.get(\"/<int:farmer_id>\")\n@farmers_bp.output(FarmerSchema) # <-- Updated\ndef get_farmer_by_id(farmer_id: int):\n    farmer = Farmer.query.where(Farmer.id == farmer_id).first()\n    if farmer is None:\n        raise HTTPError(404, message=\"Farmer not found\")\n    return farmer\n```\n\nAnd now, if we were to ping the same request as before, we get the same response! This is thanks to the\n`SQLAlchemyAutoSchema` automatically parsing all the properties of the associated `model` (passed in its `Meta` class).\nThis means any new fields added to our ORM model will be automatically added to our schema!\n\n### Relationships\n\nLet's add a new ORM model that has a many-to-one relationship with the `Farmer`, such as chickens.\n\n<Tabs>\n  <TabItem value=\"image\" label=\"Image\" default>\n    <div style={{\"text-align\": \"center\"}}>\n        ![farmer-chicken-schema](./assets/dry_with_api_flask/farmer_chicken.png)\n    </div>\n  </TabItem>\n  <TabItem value=\"code\" label=\"Code\">\n  \n    ```python\n    class Sex(enum.Enum):\n        MALE = \"male\"\n        FEMALE = \"female\"\n\n\n    class Chicken(db.Model):\n        id: Mapped[int] = mapped_column(\n            Integer, primary_key=True,\n        )\n        created_at: Mapped[DateTime] = mapped_column(\n            DateTime, nullable=False, server_default=func.now(),\n        )\n        updated_at: Mapped[DateTime] = mapped_column(\n            DateTime, nullable=False, server_default=func.now(), onupdate=func.now(),\n        )\n        farmer_id: Mapped[int] = mapped_column(\n            Integer, ForeignKey(\"farmer.id\", ondelete=\"CASCADE\"),\n        )\n        age: Mapped[int] = mapped_column(\n            Integer, nullable=False,\n        )\n        sex: Mapped[Sex] = mapped_column(\n            Enum(Sex), nullable=False,\n        )\n\n\n    class ChickenSchema(marsh.SQLAlchemyAutoSchema):\n        class Meta:\n            model = Chicken\n    ```\n    \n  </TabItem>\n</Tabs>\n\nOh no, it's starting to rain. We have duplication on some of our fields in the model (`id`, `created_at`, `updated_at`),\nbut we are seasoned developers, and we know we can just abstract that out to a `BaseModel` of sorts. No biggie!\n\n<Tabs>\n    <TabItem value=\"image\" label=\"Image\" default>\n        <div style={{\"text-align\": \"center\"}}>\n            ![farmer-chicken-schema](./assets/dry_with_api_flask/farmer_chicken_base.png)\n        </div>\n    </TabItem>\n    <TabItem value=\"code\" label=\"Code\">\n\n    ```python\n    class BaseModel(db.Model):\n        id: Mapped[int] = mapped_column(\n            Integer, primary_key=True,\n        )\n        created_at: Mapped[DateTime] = mapped_column(\n            DateTime, nullable=False, server_default=func.now(),\n        )\n        updated_at: Mapped[DateTime] = mapped_column(\n            DateTime, nullable=False, server_default=func.now(), onupdate=func.now(),\n        )\n\n        # --- METADATA ---\n        __abstract__ = True\n    ```\n\n    </TabItem>\n</Tabs>\nAnd then we just inherit from the `BaseModel` for both `Farmer` and `Chicken`. Easy! The `Farmer` class is looking very\nsimple now, which is good.\n\n```python\nclass Farmer(BaseModel):\n    name: Mapped[str] = mapped_column(\n        String, nullable=False,\n    )\n\n    # --- RELATIONSHIPS ---\n    chickens: Mapped[List[Chicken]] = relationship(\n        \"Chicken\", cascade=\"all, delete\",\n    )\n```\n\nBut what about the duplication of the `Schema` classes we are making? They are the same each time, except the\n`Meta.model` points to whichever model the schema belongs to. How could we extract this out to reduce duplication? Well,\nknow that we have a `BaseModel`, let's just give it a `classmethod` that generates our `Schema` class for us!\n\n```python\nclass BaseMeta(object):\n    include_relationships = True\n\n\nclass BaseModel(db.Model):\n    ...\n    __schema__ = None\n    \n    @classmethod\n    def make_schema(cls) -> type(SQLAlchemyAutoSchema):\n        if cls.__schema__ is not None:\n            return cls.__schema__\n        \n        meta_kwargs = {\n            \"model\": cls,\n        }\n        meta_class = type(\"Meta\", (BaseMeta,), meta_kwargs)\n        \n        schema_kwargs = {\n            \"Meta\": meta_class\n        }\n        schema_name = f\"{cls.__name__}Schema\"\n        \n        cls.__schema__ = type(schema_name, (SQLAlchemyAutoSchema,), schema_kwargs)\n        return cls.__schema__\n```\n\nThis is a pretty crafty method that creates a customer `Meta` class for the given `cls`, and then uses that in a custom\n`SQLAlchemyAutoSchema` class, which is then returned. We can now set the `FarmerSchema` and `ChickenSchema` as follows:\n\n```python\nFarmerSchema = Farmer.make_schema()\nChickenSchema = Chicken.make_schema()\n```\n\nNow, let's add a couple of chickens for the farmer in our database, and test out the same endpoint. Here is the\nresponse:\n\n```js\n{\n  \"chickens\": [\n    1,\n    2\n  ],\n  \"created_at\": \"2023-12-12T15:51:00\",\n  \"id\": 1,\n  \"name\": \"Old MacDonald\",\n  \"updated_at\": \"2023-12-12T15:51:00\"\n}\n```\n\nWhat's going on here? We have the `include_relationships` property in `FarmerSchema.Meta`, so why are we only getting\nthe `id` of each `Chicken`? Unfortunately, the way to get composition relationships in `marshmallow.Schema` is through\n`Nested` fields. There is no auto translation of `SQLAlchemy.relationship()` to `marshmallow.fields.Nested`, but we are\nclever developers, right? We can figure something out.\n\n```python\nclass BaseModel(db.Model):\n    ...\n    @classmethod\n    def get_relationship(cls, attr_name: str) -> Optional[Relationship]:\n        attr = getattr(cls, attr_name)\n        prop = getattr(attr, \"property\", None)\n        if prop is None or not isinstance(prop, Relationship):\n            return None\n        return prop\n    \n    @classmethod\n    def nest_attribute(cls, attr_name: str, prop: Relationship, schema_kwargs: dict):\n        many = getattr(prop, \"collection_class\", None) is not None\n        entity = getattr(prop, \"entity\", None)\n        nested_class = getattr(entity, \"class_\", None)\n        if not hasattr(nested_class, \"make_schema\"):\n            raise TypeError(f\"Unexpected nested type [{type(nested_class).__name__}]\")\n\n        schema_kwargs[attr_name] = fields.Nested(\n            nested_class.make_schema()(many=many)\n        )\n    \n    @classmethod\n    def make_schema(cls) -> type(SQLAlchemyAutoSchema):        \n        ... # same as before\n\n        # Add relationships to the schema\n        for attr_name in cls.__dict__:\n            if (prop := cls.get_relationship(attr_name)) is not None:\n                cls.nest_attribute(attr_name, prop, schema_kwargs)\n\n        cls.__schema__ = type(schema_name, (SQLAlchemyAutoSchema,), schema_kwargs)\n        return cls.__schema__\n```\n\nThis new `make_schema()` method will automatically detect any fields that are `SQLAlchemy.Relationships`, and convert\nthem to the appropriate `marshmallow.fields.Nested()` as long as the class inherits from `BaseModel`. Pretty nifty!\n\nNow, if we make the same request as before, let's see what we get:\n\n> TypeError: Object of type Sex is not JSON serializable\n\nNot the first time I've heard that. Let's see what we can do to fix this. The issue is very similar to the relationship\nvs. nested problem we saw before. `SQLAlchemy` has one notion of an `Enum`, while `marshmallow` has another. We can do a\nsimilar conversion within our `make_schema` function as follows:\n\n```python\nclass BaseModel(db.Model):\n    ... # same as before\n    @classmethod\n    def get_enum(cls, attr_name: str) -> Optional[Type[Enum]]:\n        attr = getattr(cls, attr_name)\n        attr_type = getattr(attr, \"type\", None)\n        if attr_type is None:\n            return None\n\n        return getattr(attr_type, \"enum_class\", None)\n\n    @classmethod\n    def enum_attribute(cls, attr_name: str, enum_class: Type[Enum], schema_kwargs: dict):\n        schema_kwargs[attr_name] = fields.Enum(enum_class)\n\n    @classmethod\n    def make_schema(cls) -> type(SQLAlchemyAutoSchema):\n        ... # same as before\n\n        for attr_name in cls.__dict__:\n            if (prop := cls.get_relationship(attr_name)) is not None:\n                cls.nest_attribute(attr_name, prop, schema_kwargs)\n            elif (enum_class := cls.get_enum(attr_name)) is not None:\n                cls.enum_attribute(attr_name, enum_class, schema_kwargs)\n\n        cls.__schema__ = type(schema_name, (SQLAlchemyAutoSchema,), schema_kwargs)\n        return cls.__schema__\n```\n\nNow, when we make the same request, we get:\n\n```js\n{\n  \"chickens\": [\n    {\n      \"age\": 3,\n      \"created_at\": \"2023-12-12T18:17:53\",\n      \"id\": 1,\n      \"sex\": \"MALE\",\n      \"updated_at\": \"2023-12-12T18:17:53\"\n    },\n    {\n      \"age\": 2,\n      \"created_at\": \"2023-12-12T18:46:30\",\n      \"id\": 2,\n      \"sex\": \"FEMALE\",\n      \"updated_at\": \"2023-12-12T18:46:30\"\n    }\n  ],\n  \"created_at\": \"2023-12-12T15:51:00\",\n  \"id\": 1,\n  \"name\": \"Old MacDonald\",\n  \"updated_at\": \"2023-12-12T15:51:00\"\n}\n```\n\n### Polymorphism\n\nNow that our relationships are healthy, we can move to the next step: polymorphism! Let's say we don't want to just keep\ntrack of farmers and their livestock, but also their crops! Well, `SQLAlchemy` has us covered with its `__mapper_args__`\nmetadata and the `polymorphic` fields of that object!\n\nFor our purposes, we want one generic `Crop` model that keeps track of the type of crop, the maturity time, and how many\nacres a farmer has of that crop.\n<Tabs>\n  <TabItem value=\"image\" label=\"Image\" default>\n    ![Crop Image](./assets/dry_with_api_flask/farmer_chick_crops.png)\n  </TabItem>\n  <TabItem value=\"code\" label=\"Code\">\n    ```python\n    class Crop(BaseModel):\n        farmer_id: Mapped[int] = mapped_column(\n            Integer, ForeignKey(\"farmer.id\", ondelete=\"CASCADE\"), nullable=False,\n        )\n        type: Mapped[str] = mapped_column(\n            String, nullable=False,\n        )\n        days_to_mature: Mapped[int] = mapped_column(\n            Integer, nullable=False,\n        )\n        acres: Mapped[float] = mapped_column(\n            Float, nullable=False,\n        )\n\n        # --- METADATA ---\n        __mapper_args__ = {\n            \"polymorphic_identity\": \"crop\",\n            \"polymorphic_on\": \"type\",\n        }\n\n\n    class Cucumber(Crop):\n        id: Mapped[int] = mapped_column(\n            Integer, ForeignKey(\"crop.id\", ondelete=\"CASCADE\"), primary_key=True,\n        )\n        for_pickling: Mapped[bool] = mapped_column(\n            Boolean, default=False, nullable=False,\n        )\n\n        # --- METADATA ---\n        __mapper_args__ = {\"polymorphic_identity\": \"cucumber\"}\n\n\n    class Tomato(Crop):\n        id: Mapped[int] = mapped_column(\n            Integer, ForeignKey(\"crop.id\", ondelete=\"CASCADE\"), primary_key=True,\n        )\n        diameter: Mapped[float] = mapped_column(\n            Float, nullable=False,\n        )\n\n        # --- METADATA ---\n        __mapper_args__ = {\"polymorphic_identity\": \"tomato\"}\n    ```\n  </TabItem>\n\n\n</Tabs>\nNow, we also want to move all of our schema declarations into their own `schemas` module. After doing that, we create\nthe `CucumberSchema` and `TomatoSchema` as normal:\n\n```python\nCucumberSchema = Cucumber.make_schema()\nTomatoSchema = Tomato.make_schema()\n```\n\nEverything is looking good, but there is trouble on the horizon. If we look at the generated schema for the `Farmer`,\nsomething is off. The `crops` field says it is a list of `CropSchemas`, but this is only partially true. Ideally, the\n`crops` field should be a list of either `TomatoSchemas` or `CucumberSchemas`.\n\n## The Magic of OneOfSchema\n\nThankfully, there is already an extension to help us solve this problem; itroducing [`marshmallow_oneofschema`](https://github.com/marshmallow-code/marshmallow-oneofschema)!\n\n### Polymorphism II: Even DRYer\n\nTo use the `OneOfSchema` class for our `CropSchema`, we just have to do the following:\n\n```python\nclass CropSchema(OneOfSchema):\n    type_schemas: Dict[str, str] = {\n        \"cucumber\": CucumberSchema,\n        \"tomato\": TomatoSchema,\n    }\n\n    type_field_remove = False\n\n    def get_obj_type(self, obj: Crop):\n        return obj.type\n```\n\nThe `type_schemas` property is a mapping of the `type` field of a given `Crop` to which schema it should use when\nserializing or deserializing. It's that simple! Unfortunately, this has one drawback when implementing into our given\nstack: `make_schema()` does not know of `CropSchema's` existence. When creating the `FarmerSchema`, it will deduce the\nclass of the `crops` field, which is `Crop`, and then it will call `Crop.make_schema()` to get the nested schema.\n\nThis is no good! What can we do to fix this? Overrides.\n\n```python\nclass BaseModel(db.Model):\n    ... # same as before\n    @classmethod\n    def make_schema(cls, overrides: Optional[Dict[str, fields.Field]] = None) -> type(SQLAlchemyAutoSchema):\n        ... # same as before\n\n        for attr_name in cls.__dict__:\n            if attr_name in overrides:\n                schema_kwargs[attr_name] = overrides[attr_name]\n            elif (prop := cls.get_relationship(attr_name)) is not None:\n                cls.nest_attribute(attr_name, prop, schema_kwargs)\n            elif (enum_class := cls.get_enum(attr_name)) is not None:\n                cls.enum_attribute(attr_name, enum_class, schema_kwargs)\n\n        cls.__schema__ = type(schema_name, (SQLAlchemyAutoSchema,), schema_kwargs)\n        return cls.__schema__\n```\n\nThis way, when we create the `FarmerSchema`, we can tell it specifically to use the polymorphic `CropSchema` for the\n`crops` field.\n\n```python\nFarmerSchema = Farmer.make_schema(\n    overrides={\"crops\": fields.Nested(CropSchema(), many=True)}\n)\n```\n\nNow, when we call our endpoint, we get:\n\n```js\n{\n  \"chickens\": [\n    {\n      \"age\": 3,\n      \"created_at\": \"2023-12-12T18:17:53\",\n      \"id\": 1,\n      \"sex\": \"MALE\",\n      \"updated_at\": \"2023-12-12T18:17:53\"\n    },\n    {\n      \"age\": 2,\n      \"created_at\": \"2023-12-12T18:46:30\",\n      \"id\": 2,\n      \"sex\": \"FEMALE\",\n      \"updated_at\": \"2023-12-12T18:46:30\"\n    }\n  ],\n  \"created_at\": \"2023-12-12T15:51:00\",\n  \"crops\": [\n    {\n      \"acres\": 1,\n      \"created_at\": \"2023-12-12T20:21:32\",\n      \"days_to_mature\": 60,\n      \"for_pickling\": true,\n      \"id\": 1,\n      \"type\": \"cucumber\",\n      \"updated_at\": \"2023-12-12T20:21:32\"\n    },\n    {\n      \"acres\": 0.5,\n      \"created_at\": \"2023-12-12T20:22:07\",\n      \"days_to_mature\": 80,\n      \"diameter\": 3,\n      \"id\": 2,\n      \"type\": \"tomato\",\n      \"updated_at\": \"2023-12-12T20:22:07\"\n    }\n  ],\n  \"id\": 1,\n  \"name\": \"Old MacDonald\",\n  \"updated_at\": \"2023-12-12T15:51:00\"\n}\n```\n\nBeautiful and dry! Like a sunny day! ‚òÄÔ∏è\n\n## Mechanics (AKA Auto-Docs)\n\nA fantastic feature of APIFlask is that it conforms to the OpenAPI spec with its routes and schemas. This means we've\nactually been documenting our APIs the whole time as we write them! Here are the docs:\n\n<iframe\n        id=\"apiflask-docs\"\n        title=\"APIFLask Docs\"\n        src=\"https://dry-apiflask-demo-r5uz5svela-uc.a.run.app/docs#/\"\n        width=\"100%\"\n        height=\"500px\"\n>\n</iframe>\n\n\n### The First 90%\n\nIf you look around the auto generated docs, you'll see the routes that we made, as well as the schemas that are in use.\nOne quick change I'd suggest is to try out all the different UIs available for the docs site. You can update this by\nsetting the `docs_ui` key-word argument in the `APIFlask` constructor like so:\n\n```python\nAPIFlask(__name__, title=\"DRY API\", version=\"1.0\", docs_ui=\"elements\")\n```\n\nDevelopers with sharp eyes may notice that the `Crop` schema doesn't have any information populated in our docs! This is\na problem.\n\n### The Last 10%\n\nThe final savior: [`apispec_oneofschema`](https://github.com/timakro/apispec-oneofschema), a companion to `marshmallow_oneofschema`. This plugin allows us to generate\ndocumentation for our `OneOfSchema` schemas. Let's set it up now!\n\nIt's as simple as changing this:\n\n```python\napp = APIFlask(__name__, title=\"DRY API\", version=\"1.0\", docs_ui=\"elements\")\n```\n\nTo this:\n\n```python\napp = APIFlask(__name__, title=\"DRY API\", version=\"1.0\", docs_ui=\"elements\", spec_plugins=[MarshmallowPlugin()])\n```\n\n### The last 1%\n\nLastly, the `oneOf` dropdown for most of the UIs just says `object` for each option, which isn't great. From what I can\ntell, most of the UIs use the `title` field of a schema to populate the name, so we can create our own plugin to add\nthat field for each of our schemas:\n\n```python\nfrom apispec.ext import marshmallow\n\n\nclass OpenAPITitleAppender(marshmallow.OpenAPIConverter):\n    def schema2jsonschema(self, schema):\n        json_schema = super(OpenAPITitleAppender, self).schema2jsonschema(schema)\n        schema_name = schema.__class__.__name__\n        if schema_name.endswith('Schema'):\n            schema_name = schema_name[:-len('Schema')]\n        json_schema[\"title\"] = schema_name\n        return json_schema\n\n\nclass TitlesPlugin(marshmallow.MarshmallowPlugin):\n    Converter = OpenAPITitleAppender\n```\n\nAnd then we just have to add it to our `APIFlask` app!\n\n```python\napp = APIFlask(\n    __name__,\n    title=\"DRY API\",\n    version=\"1.0\",\n    docs_ui=\"elements\",\n    spec_plugins=[MarshmallowPlugin(), TitlesPlugin()]\n)\n```"}]}}